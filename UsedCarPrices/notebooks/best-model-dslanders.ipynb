{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3a7b5f",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-09-05T20:02:14.983014Z",
     "iopub.status.busy": "2024-09-05T20:02:14.982632Z",
     "iopub.status.idle": "2024-09-05T20:04:17.317998Z",
     "shell.execute_reply": "2024-09-05T20:04:17.316409Z"
    },
    "papermill": {
     "duration": 122.345483,
     "end_time": "2024-09-05T20:04:17.321459",
     "exception": false,
     "start_time": "2024-09-05T20:02:14.975976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray==2.10.0\r\n",
      "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.15.1)\r\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (4.22.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.0.8)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.4.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (2.32.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.18.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.10.0) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (2024.7.4)\r\n",
      "Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ray\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.24.0\r\n",
      "    Uninstalling ray-2.24.0:\r\n",
      "      Successfully uninstalled ray-2.24.0\r\n",
      "Successfully installed ray-2.10.0\r\n",
      "Collecting autogluon.tabular\r\n",
      "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (1.26.4)\r\n",
      "Collecting scipy<1.13,>=1.5.4 (from autogluon.tabular)\r\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m943.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (2.2.2)\r\n",
      "Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.tabular)\r\n",
      "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (3.3)\r\n",
      "Collecting autogluon.core==1.1.1 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting autogluon.features==1.1.1 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (4.66.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (2.32.3)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (3.7.5)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (1.26.100)\r\n",
      "Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.tabular)\r\n",
      "  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (5.9.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (70.0.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.5.0)\r\n",
      "Collecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\r\n",
      "  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (0.6.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (2024.7.4)\r\n",
      "Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scipy, scikit-learn, botocore, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.14.0\r\n",
      "    Uninstalling scipy-1.14.0:\r\n",
      "      Successfully uninstalled scipy-1.14.0\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: botocore\r\n",
      "    Found existing installation: botocore 1.34.131\r\n",
      "    Uninstalling botocore-1.34.131:\r\n",
      "      Successfully uninstalled botocore-1.34.131\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.tabular-1.1.1 botocore-1.29.165 scikit-learn-1.4.0 scipy-1.12.0\r\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\r\n",
      "Collecting ipywidgets\r\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.21.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\r\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\r\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\r\n",
      "  Attempting uninstall: widgetsnbextension\r\n",
      "    Found existing installation: widgetsnbextension 3.6.8\r\n",
      "    Uninstalling widgetsnbextension-3.6.8:\r\n",
      "      Successfully uninstalled widgetsnbextension-3.6.8\r\n",
      "  Attempting uninstall: jupyterlab-widgets\r\n",
      "    Found existing installation: jupyterlab_widgets 3.0.11\r\n",
      "    Uninstalling jupyterlab_widgets-3.0.11:\r\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.11\r\n",
      "  Attempting uninstall: ipywidgets\r\n",
      "    Found existing installation: ipywidgets 7.7.1\r\n",
      "    Uninstalling ipywidgets-7.7.1:\r\n",
      "      Successfully uninstalled ipywidgets-7.7.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ray==2.10.0\n",
    "!pip install autogluon.tabular\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf34326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:17.509902Z",
     "iopub.status.busy": "2024-09-05T20:04:17.508910Z",
     "iopub.status.idle": "2024-09-05T20:04:21.623312Z",
     "shell.execute_reply": "2024-09-05T20:04:21.622371Z"
    },
    "papermill": {
     "duration": 4.201812,
     "end_time": "2024-09-05T20:04:21.625923",
     "exception": false,
     "start_time": "2024-09-05T20:04:17.424111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b816d54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:21.768527Z",
     "iopub.status.busy": "2024-09-05T20:04:21.767442Z",
     "iopub.status.idle": "2024-09-05T20:04:23.417678Z",
     "shell.execute_reply": "2024-09-05T20:04:23.416614Z"
    },
    "papermill": {
     "duration": 1.726578,
     "end_time": "2024-09-05T20:04:23.420294",
     "exception": false,
     "start_time": "2024-09-05T20:04:21.693716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv').drop('id', axis=1)\n",
    "main = pd.read_csv('/kaggle/input/used-car-price-prediction-dataset/used_cars.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv').drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8085a8ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:23.557754Z",
     "iopub.status.busy": "2024-09-05T20:04:23.556876Z",
     "iopub.status.idle": "2024-09-05T20:04:23.591210Z",
     "shell.execute_reply": "2024-09-05T20:04:23.590089Z"
    },
    "papermill": {
     "duration": 0.106818,
     "end_time": "2024-09-05T20:04:23.593927",
     "exception": false,
     "start_time": "2024-09-05T20:04:23.487109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main['price'] = main['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "main['milage'] = main['milage'].str.replace('mi.', '').str.replace(',', '').astype(float)\n",
    "\n",
    "total = pd.concat([train, main], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4cd6e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:23.732682Z",
     "iopub.status.busy": "2024-09-05T20:04:23.732271Z",
     "iopub.status.idle": "2024-09-05T20:04:23.758892Z",
     "shell.execute_reply": "2024-09-05T20:04:23.757987Z"
    },
    "papermill": {
     "duration": 0.098333,
     "end_time": "2024-09-05T20:04:23.761242",
     "exception": false,
     "start_time": "2024-09-05T20:04:23.662909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MINI</td>\n",
       "      <td>Cooper S Base</td>\n",
       "      <td>2007</td>\n",
       "      <td>213000.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>LS V8</td>\n",
       "      <td>2002</td>\n",
       "      <td>143250.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Beige</td>\n",
       "      <td>At least 1 accident or damage reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Silverado 2500 LT</td>\n",
       "      <td>2002</td>\n",
       "      <td>136731.0</td>\n",
       "      <td>E85 Flex Fuel</td>\n",
       "      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>G90 5.0 Ultimate</td>\n",
       "      <td>2017</td>\n",
       "      <td>19500.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>Transmission w/Dual Shift Mode</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>45000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>Metris Base</td>\n",
       "      <td>2021</td>\n",
       "      <td>7388.0</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>7-Speed A/T</td>\n",
       "      <td>Black</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>97500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           brand              model  model_year    milage      fuel_type  \\\n",
       "0           MINI      Cooper S Base        2007  213000.0       Gasoline   \n",
       "1        Lincoln              LS V8        2002  143250.0       Gasoline   \n",
       "2      Chevrolet  Silverado 2500 LT        2002  136731.0  E85 Flex Fuel   \n",
       "3        Genesis   G90 5.0 Ultimate        2017   19500.0       Gasoline   \n",
       "4  Mercedes-Benz        Metris Base        2021    7388.0       Gasoline   \n",
       "\n",
       "                                              engine  \\\n",
       "0       172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n",
       "1       252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n",
       "2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...   \n",
       "3       420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n",
       "4       208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n",
       "\n",
       "                     transmission ext_col int_col  \\\n",
       "0                             A/T  Yellow    Gray   \n",
       "1                             A/T  Silver   Beige   \n",
       "2                             A/T    Blue    Gray   \n",
       "3  Transmission w/Dual Shift Mode   Black   Black   \n",
       "4                     7-Speed A/T   Black   Beige   \n",
       "\n",
       "                                 accident clean_title    price  \n",
       "0                           None reported         Yes   4200.0  \n",
       "1  At least 1 accident or damage reported         Yes   4999.0  \n",
       "2                           None reported         Yes  13900.0  \n",
       "3                           None reported         Yes  45000.0  \n",
       "4                           None reported         Yes  97500.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6656428e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:23.903292Z",
     "iopub.status.busy": "2024-09-05T20:04:23.902322Z",
     "iopub.status.idle": "2024-09-05T20:04:23.918326Z",
     "shell.execute_reply": "2024-09-05T20:04:23.917381Z"
    },
    "papermill": {
     "duration": 0.085013,
     "end_time": "2024-09-05T20:04:23.920474",
     "exception": false,
     "start_time": "2024-09-05T20:04:23.835461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Land</td>\n",
       "      <td>Rover LR2 Base</td>\n",
       "      <td>2015</td>\n",
       "      <td>98000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>6-Speed A/T</td>\n",
       "      <td>White</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Land</td>\n",
       "      <td>Rover Defender SE</td>\n",
       "      <td>2020</td>\n",
       "      <td>9142</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...</td>\n",
       "      <td>8-Speed A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford</td>\n",
       "      <td>Expedition Limited</td>\n",
       "      <td>2022</td>\n",
       "      <td>28121</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n",
       "      <td>10-Speed Automatic</td>\n",
       "      <td>White</td>\n",
       "      <td>Ebony</td>\n",
       "      <td>None reported</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audi</td>\n",
       "      <td>A6 2.0T Sport</td>\n",
       "      <td>2016</td>\n",
       "      <td>61258</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>2.0 Liter TFSI</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Silician Yellow</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audi</td>\n",
       "      <td>A6 2.0T Premium Plus</td>\n",
       "      <td>2018</td>\n",
       "      <td>59000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand                 model  model_year  milage fuel_type  \\\n",
       "0  Land        Rover LR2 Base        2015   98000  Gasoline   \n",
       "1  Land     Rover Defender SE        2020    9142    Hybrid   \n",
       "2  Ford    Expedition Limited        2022   28121  Gasoline   \n",
       "3  Audi         A6 2.0T Sport        2016   61258  Gasoline   \n",
       "4  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n",
       "\n",
       "                                              engine        transmission  \\\n",
       "0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n",
       "1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n",
       "2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n",
       "3                                     2.0 Liter TFSI           Automatic   \n",
       "4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n",
       "\n",
       "           ext_col int_col       accident clean_title  \n",
       "0            White   Beige  None reported         Yes  \n",
       "1           Silver   Black  None reported         Yes  \n",
       "2            White   Ebony  None reported         NaN  \n",
       "3  Silician Yellow   Black  None reported         NaN  \n",
       "4             Gray   Black  None reported         Yes  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b118eb6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:24.063519Z",
     "iopub.status.busy": "2024-09-05T20:04:24.062577Z",
     "iopub.status.idle": "2024-09-05T20:04:24.073108Z",
     "shell.execute_reply": "2024-09-05T20:04:24.072028Z"
    },
    "papermill": {
     "duration": 0.085983,
     "end_time": "2024-09-05T20:04:24.075445",
     "exception": false,
     "start_time": "2024-09-05T20:04:23.989462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "\n",
    "    # Feature 1: Age of the vehicle\n",
    "    df['age'] = 2024 - df['model_year']\n",
    "\n",
    "    # Feature 2: Mileage per year\n",
    "    df['milage_per_year'] = df['milage'] / df['age']\n",
    "    inf_mask = df['milage_per_year'].replace([float('inf'), -float('inf')], np.nan).isna()\n",
    "    df.loc[inf_mask, 'milage_per_year'] = df.loc[inf_mask, 'milage'] / (df.loc[inf_mask, 'age'] + 0.75)\n",
    "    \n",
    "    # Feature 3: Extracting horsepower (HP) from the engine column\n",
    "    df['horsepower'] = df['engine'].str.extract(r'(\\d+\\.\\d+)HP').astype(float)\n",
    "\n",
    "    # Feature 4: Extracting engine displacement (L) from the engine column\n",
    "    df['engine_displacement'] = df['engine'].str.extract(r'(\\d+\\.\\d+)L').astype(float)\n",
    "\n",
    "    # Feature 5: Extracting the number of cylinders from the engine column\n",
    "    df['number_cylinders'] = df['engine'].str.extract(r'(\\d+ Cylinder)').astype(str)\n",
    "    df['number_cylinders'] = df['number_cylinders'].replace('nan', np.nan)\n",
    "    df['number_cylinders'] = df['number_cylinders'].str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "    # Feature 6: Extracting the type of fuel from the engine column\n",
    "    df['engine_fuel_type'] = df['engine'].str.extract(r'Engine (.+ Fuel)').astype(str)\n",
    "    df['engine_fuel_type'] = df['engine_fuel_type'].replace('nan', np.nan)\n",
    "    \n",
    "    df = df.drop(['engine', 'model_year'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "750c06bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:24.215417Z",
     "iopub.status.busy": "2024-09-05T20:04:24.214849Z",
     "iopub.status.idle": "2024-09-05T20:04:24.222078Z",
     "shell.execute_reply": "2024-09-05T20:04:24.221171Z"
    },
    "papermill": {
     "duration": 0.079793,
     "end_time": "2024-09-05T20:04:24.224148",
     "exception": false,
     "start_time": "2024-09-05T20:04:24.144355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def null_handling(df):\n",
    "        \n",
    "    df['horsepower'] = df['horsepower'].fillna(df['horsepower'].mean())\n",
    "    df['engine_displacement'] = df['engine_displacement'].fillna(df['engine_displacement'].mean())\n",
    "    df['number_cylinders'] = df['number_cylinders'].fillna(df['number_cylinders'].mode()[0])\n",
    "    \n",
    "    df['fuel_type'] = df['fuel_type'].fillna('missing')\n",
    "    df['accident'] = df['accident'].fillna('missing')\n",
    "    df['engine_fuel_type'] = df['engine_fuel_type'].fillna('missing')\n",
    "    \n",
    "#     df['fuel_type'] = df['fuel_type'].replace('–', df['fuel_type'].mode()[0])\n",
    "#     df['transmission'] = df['transmission'].replace('–', df['fuel_type'].mode()[0])\n",
    "    df['clean_title'] = df['clean_title'].fillna('No')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b956036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:24.413375Z",
     "iopub.status.busy": "2024-09-05T20:04:24.412953Z",
     "iopub.status.idle": "2024-09-05T20:04:24.432303Z",
     "shell.execute_reply": "2024-09-05T20:04:24.431151Z"
    },
    "papermill": {
     "duration": 0.143935,
     "end_time": "2024-09-05T20:04:24.434618",
     "exception": false,
     "start_time": "2024-09-05T20:04:24.290683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encoding(df_train, df_test):\n",
    "    # Encode interior color based on frequency\n",
    "    int_color_freq = df_train['int_col'].value_counts(normalize=True)\n",
    "    df_train['int_col'] = df_train['int_col'].map(int_color_freq)\n",
    "    df_test['int_col'] = df_test['int_col'].map(int_color_freq)\n",
    "    \n",
    "    # Encode exterior color based on frequency\n",
    "    ext_color_freq = df_train['ext_col'].value_counts(normalize=True)\n",
    "    df_train['ext_col'] = df_train['ext_col'].map(ext_color_freq)\n",
    "    df_test['ext_col'] = df_test['ext_col'].map(ext_color_freq)\n",
    "    \n",
    "    # Encode models based on frequency\n",
    "    model_freq = df_train['model'].value_counts(normalize=True)\n",
    "    df_train['model'] = df_train['model'].map(model_freq)\n",
    "    df_test['model'] = df_test['model'].map(model_freq)\n",
    "    \n",
    "    # Encode brands based on frequency\n",
    "    brand_freq = df_train['brand'].value_counts(normalize=True)\n",
    "    df_train['brand'] = df_train['brand'].map(brand_freq)\n",
    "    df_test['brand'] = df_test['brand'].map(brand_freq)\n",
    "    \n",
    "    # Extract transmission type from the transmission column\n",
    "    def extract_transmission_type(transmission):\n",
    "        if 'Automatic' in transmission:\n",
    "            return 'Automatic'\n",
    "        elif 'Manual' in transmission:\n",
    "            return 'Manual'\n",
    "        elif 'CVT' in transmission:\n",
    "            return 'CVT'\n",
    "        elif 'DCT' in transmission:\n",
    "            return 'DCT'\n",
    "        elif 'Fixed Gear' in transmission:\n",
    "            return 'Fixed Gear'\n",
    "        elif 'Variable' in transmission:\n",
    "            return 'Variable'\n",
    "        elif 'Single-Speed' in transmission:\n",
    "            return 'Single-Speed'\n",
    "        else:\n",
    "            return 'None'\n",
    "\n",
    "    # Apply transmission type extraction to both train and test sets\n",
    "    df_train['transmission_type'] = df_train['transmission'].apply(extract_transmission_type)\n",
    "    df_test['transmission_type'] = df_test['transmission'].apply(extract_transmission_type)\n",
    "\n",
    "    # Extract speed count from the transmission column\n",
    "    def extract_speed_count(transmission):\n",
    "        for speed in ['1', '2', '4', '5', '6', '7', '8', '9', '10']:\n",
    "            if f'{speed}-Speed' in transmission:\n",
    "                return speed\n",
    "        return 'None'\n",
    "\n",
    "    # Apply speed count extraction to both train and test sets\n",
    "    df_train['speed_count'] = df_train['transmission'].apply(extract_speed_count)\n",
    "    df_test['speed_count'] = df_test['transmission'].apply(extract_speed_count)\n",
    "\n",
    "    # Encode transmission type using LabelEncoder\n",
    "    type_encoder = LabelEncoder()\n",
    "    df_train['transmission_type'] = type_encoder.fit_transform(df_train['transmission_type'])\n",
    "    df_test['transmission_type'] = type_encoder.transform(df_test['transmission_type'])\n",
    "    \n",
    "    # Encode speed count using LabelEncoder\n",
    "    speed_encoder = LabelEncoder()\n",
    "    df_train['speed_count'] = speed_encoder.fit_transform(df_train['speed_count'])\n",
    "    df_test['speed_count'] = speed_encoder.transform(df_test['speed_count'])\n",
    "    \n",
    "    # Drop the original transmission column\n",
    "    df_train = df_train.drop(['transmission'], axis=1)\n",
    "    df_test = df_test.drop(['transmission'], axis=1)\n",
    "    \n",
    "    # One-hot encode remaining categorical features\n",
    "    one_hot = OneHotEncoder(drop='first', sparse_output=False)\n",
    "    cat_cols = [col for col in df_train.columns if df_train[col].dtype == 'object']\n",
    "\n",
    "    # Fit and transform the train set, transform the test set\n",
    "    encoded_train = one_hot.fit_transform(df_train[cat_cols])\n",
    "    encoded_test = one_hot.transform(df_test[cat_cols])\n",
    "\n",
    "    # Convert the encoded arrays to DataFrames with proper column names\n",
    "    encoded_train_df = pd.DataFrame(encoded_train, \n",
    "                                    columns=one_hot.get_feature_names_out(cat_cols), \n",
    "                                    index=df_train.index)\n",
    "    encoded_test_df = pd.DataFrame(encoded_test, \n",
    "                                   columns=one_hot.get_feature_names_out(cat_cols), \n",
    "                                   index=df_test.index)\n",
    "\n",
    "    # Drop original categorical columns and concatenate the encoded columns\n",
    "    df_train = df_train.drop(cat_cols, axis=1)\n",
    "    df_train = pd.concat([df_train, encoded_train_df], axis=1)\n",
    "\n",
    "    df_test = df_test.drop(cat_cols, axis=1)\n",
    "    df_test = pd.concat([df_test, encoded_test_df], axis=1)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546d1d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:24.572690Z",
     "iopub.status.busy": "2024-09-05T20:04:24.572206Z",
     "iopub.status.idle": "2024-09-05T20:04:32.049177Z",
     "shell.execute_reply": "2024-09-05T20:04:32.048029Z"
    },
    "papermill": {
     "duration": 7.548935,
     "end_time": "2024-09-05T20:04:32.051993",
     "exception": false,
     "start_time": "2024-09-05T20:04:24.503058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total = feature_engineering(total)\n",
    "total = null_handling(total)\n",
    "\n",
    "test = feature_engineering(test)\n",
    "test = null_handling(test)\n",
    "\n",
    "total, test = encoding(total, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "129b21cf",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-09-05T20:04:32.191620Z",
     "iopub.status.busy": "2024-09-05T20:04:32.191218Z",
     "iopub.status.idle": "2024-09-06T01:04:45.834534Z",
     "shell.execute_reply": "2024-09-06T01:04:45.833331Z"
    },
    "papermill": {
     "duration": 18013.715206,
     "end_time": "2024-09-06T01:04:45.836946",
     "exception": false,
     "start_time": "2024-09-05T20:04:32.121740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240905_200432\"\n",
      "2024-09-05 20:04:36,018\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\u001b[36m(_ray_fit pid=471)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=470)\u001b[0m 1 warning generated.\n",
      "\u001b[36m(_ray_fit pid=470)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=471)\u001b[0m 1 warning generated.\u001b[32m [repeated 52x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=541)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=471)\u001b[0m 1 warning generated.\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=610)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=680)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=715)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=841)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=912)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=982)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1052)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1228)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1228)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1308)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1308)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1396)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1396)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1485)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1485)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=1687)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=1687)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1687)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1520)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=1520)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1686)\u001b[0m No improvement since epoch 6: early stopping\n",
      "\u001b[36m(_ray_fit pid=1686)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1686)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1805)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=1805)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1805)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1765)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=1765)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1765)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1845)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=1845)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1845)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1885)\u001b[0m No improvement since epoch 3: early stopping\n",
      "\u001b[36m(_ray_fit pid=1885)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1885)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1965)\u001b[0m No improvement since epoch 1: early stopping\n",
      "\u001b[36m(_ray_fit pid=1965)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1965)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=1925)\u001b[0m No improvement since epoch 5: early stopping\n",
      "\u001b[36m(_ray_fit pid=1925)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=1925)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:22:56] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:22:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:22:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=2096)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:01] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2159)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2157)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2157)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2157)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2157)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2157)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2278)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2278)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2278)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2278)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2278)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2430)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m Potential solutions:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:23:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2280)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2498)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2568)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2638)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2801)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=2801)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2880)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2880)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2969)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=2969)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3057)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3057)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3093)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=3247)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3315)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3385)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3457)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3618)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=3618)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3618)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=3485)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=3700)\u001b[0m No improvement since epoch 3: early stopping\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3700)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3700)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=3697)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=3697)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3697)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=3778)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=3778)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3778)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=3819)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=3819)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3819)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=3859)\u001b[0m No improvement since epoch 4: early stopping\n",
      "\u001b[36m(_ray_fit pid=3859)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3859)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=3899)\u001b[0m No improvement since epoch 2: early stopping\n",
      "\u001b[36m(_ray_fit pid=3899)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=3899)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=4030)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4030)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4121)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4121)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4210)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4210)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4292)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4292)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4478)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4300)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4547)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4575)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4617)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4652)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4680)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4722)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:38] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=4849)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=4848)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=4848)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=4848)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=4848)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=4848)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:44] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:47] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4909)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:51] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:53] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=4971)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:57] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m Potential solutions:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5032)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5201)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5201)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:41:58] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:42:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:42:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=5060)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=5284)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5284)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5371)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5371)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5460)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5460)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5646)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5646)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=5492)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=5492)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5727)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5727)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5807)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5807)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5891)\u001b[0m No improvement since epoch 7: early stopping\n",
      "\u001b[36m(_ray_fit pid=5891)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=5891)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6061)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=5890)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=5890)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6144)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6144)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6233)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6323)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6323)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6524)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6357)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=6592)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6663)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6733)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6768)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=6894)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\u001b[36m(_ray_fit pid=6894)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=6894)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=6973)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6973)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=6973)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7057)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7057)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7057)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7137)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7137)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7137)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:36] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
      "\u001b[36m(_ray_fit pid=7139)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\u001b[36m(_ray_fit pid=7139)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\u001b[36m(_ray_fit pid=7139)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:36] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m Potential solutions:\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
      "\u001b[36m(_ray_fit pid=7310)\u001b[0m This warning will only be shown once.\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:40] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7370)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7370)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7370)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7370)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m Potential solutions:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7372)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:48] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7491)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7491)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7491)\u001b[0m \u001b[32m [repeated 18x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7491)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7642)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:49] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m Potential solutions:\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m - Set the device for booster before call to inplace_predict.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m This warning will only be shown once.\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:48] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [20:53:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7493)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7711)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7781)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=7853)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8014)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8014)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8101)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8101)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8182)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8182)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8190)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=8190)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=8461)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8302)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8529)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8602)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8673)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8840)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8909)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=8981)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9054)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9236)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=9236)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9317)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9317)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9414)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9414)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9495)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=9495)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                          model     score_val              eval_metric  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0           WeightedEnsemble_L3 -72303.632284  root_mean_squared_error     278.316629  10167.588415                0.002995           0.667951            3       True         53\n",
      "1        NeuralNetFastAI_BAG_L2 -72352.391728  root_mean_squared_error     258.165104   9724.602528                3.461097        1007.269429            2       True         46\n",
      "2           WeightedEnsemble_L2 -72443.123849  root_mean_squared_error      71.933454    744.880340                0.003415           0.690237            2       True         40\n",
      "3             LightGBMXT_BAG_L2 -72488.179841  root_mean_squared_error     256.148223   8758.280603                1.444216          40.947503            2       True         41\n",
      "4               CatBoost_BAG_L2 -72511.062070  root_mean_squared_error     254.913498   8751.373427                0.209491          34.040328            2       True         44\n",
      "5          CatBoost_r177_BAG_L2 -72517.248228  root_mean_squared_error     254.921645   8748.413659                0.217638          31.080560            2       True         49\n",
      "6          LightGBM_r130_BAG_L1 -72606.281582  root_mean_squared_error       1.677533     35.496591                1.677533          35.496591            1       True         23\n",
      "7            XGBoost_r98_BAG_L1 -72630.840232  root_mean_squared_error       0.346014     41.163084                0.346014          41.163084            1       True         38\n",
      "8          LightGBM_r131_BAG_L2 -72654.138262  root_mean_squared_error     257.745216   8780.650135                3.041209          63.317035            2       True         50\n",
      "9           LightGBM_r96_BAG_L1 -72655.608282  root_mean_squared_error      47.997775    222.654808               47.997775         222.654808            1       True         13\n",
      "10         LightGBM_r131_BAG_L1 -72661.922313  root_mean_squared_error       5.737383     61.007603                5.737383          61.007603            1       True         10\n",
      "11              LightGBM_BAG_L2 -72672.297531  root_mean_squared_error     255.424848   8753.740922                0.720841          36.407823            2       True         42\n",
      "12          CatBoost_r13_BAG_L1 -72680.646528  root_mean_squared_error       0.293932     38.037250                0.293932          38.037250            1       True         18\n",
      "13              LightGBM_BAG_L1 -72691.492496  root_mean_squared_error       1.333035     29.595731                1.333035          29.595731            1       True          2\n",
      "14          LightGBM_r15_BAG_L1 -72698.648222  root_mean_squared_error       2.965306     38.259015                2.965306          38.259015            1       True         39\n",
      "15           XGBoost_r89_BAG_L1 -72710.968163  root_mean_squared_error       0.304869     15.494084                0.304869          15.494084            1       True         22\n",
      "16           CatBoost_r9_BAG_L1 -72721.913939  root_mean_squared_error       0.600352     27.468683                0.600352          27.468683            1       True         12\n",
      "17         CatBoost_r167_BAG_L1 -72726.371575  root_mean_squared_error       0.114746     22.891634                0.114746          22.891634            1       True         36\n",
      "18            LightGBMXT_BAG_L1 -72735.852678  root_mean_squared_error       3.379681     42.352328                3.379681          42.352328            1       True          1\n",
      "19          CatBoost_r70_BAG_L1 -72740.768167  root_mean_squared_error       0.602907     24.576872                0.602907          24.576872            1       True         32\n",
      "20              CatBoost_BAG_L1 -72753.694478  root_mean_squared_error       0.160613     24.576001                0.160613          24.576001            1       True          4\n",
      "21          CatBoost_r50_BAG_L1 -72755.991801  root_mean_squared_error       0.521612     22.560761                0.521612          22.560761            1       True         24\n",
      "22         CatBoost_r177_BAG_L1 -72757.796818  root_mean_squared_error       0.154663     22.847212                0.154663          22.847212            1       True          9\n",
      "23  NeuralNetFastAI_r191_BAG_L2 -72784.409387  root_mean_squared_error     258.364211   8824.889261                3.660204         107.556161            2       True         51\n",
      "24         LightGBM_r196_BAG_L1 -72804.733726  root_mean_squared_error      91.777832    420.007644               91.777832         420.007644            1       True         34\n",
      "25          CatBoost_r69_BAG_L1 -72807.804204  root_mean_squared_error       0.209677     24.642840                0.209677          24.642840            1       True         28\n",
      "26         CatBoost_r137_BAG_L1 -72812.166025  root_mean_squared_error       0.187226     26.861402                0.187226          26.861402            1       True         16\n",
      "27         LightGBM_r161_BAG_L1 -72828.146962  root_mean_squared_error       6.803094     85.465630                6.803094          85.465630            1       True         30\n",
      "28               XGBoost_BAG_L1 -72837.204400  root_mean_squared_error       0.279326     14.572476                0.279326          14.572476            1       True          7\n",
      "29         LightGBM_r188_BAG_L1 -72891.124148  root_mean_squared_error       3.227483     51.102482                3.227483          51.102482            1       True         20\n",
      "30          XGBoost_r194_BAG_L1 -72899.093438  root_mean_squared_error       0.228529     14.631535                0.228529          14.631535            1       True         26\n",
      "31               XGBoost_BAG_L2 -72942.141180  root_mean_squared_error     255.711469   8743.779289                1.007462          26.446189            2       True         47\n",
      "32  NeuralNetFastAI_r102_BAG_L1 -73016.774516  root_mean_squared_error       0.859031    185.469967                0.859031         185.469967            1       True         17\n",
      "33         LightGBMLarge_BAG_L2 -73054.396311  root_mean_squared_error     256.002130   8766.411779                1.298123          49.078679            2       True         48\n",
      "34  NeuralNetFastAI_r156_BAG_L1 -73083.726356  root_mean_squared_error       0.883483    235.817507                0.883483         235.817507            1       True         33\n",
      "35       ExtraTrees_r172_BAG_L1 -73130.205840  root_mean_squared_error       8.284728     65.775203                8.284728          65.775203            1       True         27\n",
      "36         LightGBMLarge_BAG_L1 -73131.206435  root_mean_squared_error       1.184823     33.734397                1.184823          33.734397            1       True          8\n",
      "37           XGBoost_r33_BAG_L1 -73214.095855  root_mean_squared_error       0.366046     24.924384                0.366046          24.924384            1       True         14\n",
      "38  NeuralNetFastAI_r143_BAG_L1 -73388.742613  root_mean_squared_error       1.217153    375.614000                1.217153         375.614000            1       True         31\n",
      "39       NeuralNetFastAI_BAG_L1 -73557.947568  root_mean_squared_error       3.050364    778.017476                3.050364         778.017476            1       True          6\n",
      "40   NeuralNetFastAI_r95_BAG_L1 -73645.711721  root_mean_squared_error       5.963442    370.719763                5.963442         370.719763            1       True         37\n",
      "41  NeuralNetFastAI_r191_BAG_L1 -73695.722162  root_mean_squared_error       3.203273    893.101605                3.203273         893.101605            1       True         11\n",
      "42  NeuralNetFastAI_r145_BAG_L1 -73832.019416  root_mean_squared_error       6.074331   1642.069553                6.074331        1642.069553            1       True         21\n",
      "43  NeuralNetFastAI_r103_BAG_L1 -73903.021749  root_mean_squared_error       3.068908    739.414387                3.068908         739.414387            1       True         29\n",
      "44         ExtraTreesMSE_BAG_L2 -73907.626798  root_mean_squared_error     268.740654   8984.701182               14.036647         267.368083            2       True         45\n",
      "45      RandomForest_r39_BAG_L1 -74342.345926  root_mean_squared_error      10.847727    133.968160               10.847727         133.968160            1       True         35\n",
      "46       RandomForestMSE_BAG_L2 -74776.010589  root_mean_squared_error     271.790491  11388.137892               17.086484        2670.804793            2       True         43\n",
      "47        ExtraTrees_r42_BAG_L1 -75980.972201  root_mean_squared_error       7.987098     54.243201                7.987098          54.243201            1       True         15\n",
      "48         ExtraTreesMSE_BAG_L1 -76193.257067  root_mean_squared_error       7.478927     60.880453                7.478927          60.880453            1       True          5\n",
      "49   NeuralNetFastAI_r11_BAG_L1 -76215.504412  root_mean_squared_error       5.578873   1515.220197                5.578873        1515.220197            1       True         25\n",
      "50     RandomForest_r195_BAG_L1 -76464.995009  root_mean_squared_error      11.702653    149.068491               11.702653         149.068491            1       True         19\n",
      "51       RandomForestMSE_BAG_L1 -77363.645271  root_mean_squared_error       8.049561    153.028691                8.049561         153.028691            1       True          3\n",
      "52           CatBoost_r9_BAG_L2 -77625.651323  root_mean_squared_error     254.989763   8748.565766                0.285756          31.232667            2       True         52\n",
      "Number of models trained: 53\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     :  9 | ['brand', 'model', 'milage', 'ext_col', 'int_col', ...]\n",
      "('int', [])       :  3 | ['age', 'transmission_type', 'speed_count']\n",
      "('int', ['bool']) : 13 | ['fuel_type_E85 Flex Fuel', 'fuel_type_Gasoline', 'fuel_type_Hybrid', 'fuel_type_Plug-In Hybrid', 'fuel_type_missing', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20240905_200432SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "                            label='price', \n",
    "                            eval_metric ='rmse',\n",
    "                            problem_type=\"regression\").fit(total, \n",
    "                                                           presets='best_quality',\n",
    "                                                           time_limit=3600*5,\n",
    "                                                           verbosity=0,\n",
    "                                                           excluded_model_types=['KNN', 'NN_TORCH'],\n",
    "                                                           ag_args_fit={'num_gpus': 1})\n",
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe1f40cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T01:04:45.996811Z",
     "iopub.status.busy": "2024-09-06T01:04:45.995739Z",
     "iopub.status.idle": "2024-09-06T01:04:46.046576Z",
     "shell.execute_reply": "2024-09-06T01:04:46.045443Z"
    },
    "papermill": {
     "duration": 0.135494,
     "end_time": "2024-09-06T01:04:46.049599",
     "exception": false,
     "start_time": "2024-09-06T01:04:45.914105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-72303.632284</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>278.316629</td>\n",
       "      <td>10167.588415</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.667951</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetFastAI_BAG_L2</td>\n",
       "      <td>-72352.391728</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>258.165104</td>\n",
       "      <td>9724.602528</td>\n",
       "      <td>3.461097</td>\n",
       "      <td>1007.269429</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-72443.123849</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>71.933454</td>\n",
       "      <td>744.880340</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>0.690237</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMXT_BAG_L2</td>\n",
       "      <td>-72488.179841</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>256.148223</td>\n",
       "      <td>8758.280603</td>\n",
       "      <td>1.444216</td>\n",
       "      <td>40.947503</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CatBoost_BAG_L2</td>\n",
       "      <td>-72511.062070</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>254.913498</td>\n",
       "      <td>8751.373427</td>\n",
       "      <td>0.209491</td>\n",
       "      <td>34.040328</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost_r177_BAG_L2</td>\n",
       "      <td>-72517.248228</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>254.921645</td>\n",
       "      <td>8748.413659</td>\n",
       "      <td>0.217638</td>\n",
       "      <td>31.080560</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM_r130_BAG_L1</td>\n",
       "      <td>-72606.281582</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.677533</td>\n",
       "      <td>35.496591</td>\n",
       "      <td>1.677533</td>\n",
       "      <td>35.496591</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost_r98_BAG_L1</td>\n",
       "      <td>-72630.840232</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.346014</td>\n",
       "      <td>41.163084</td>\n",
       "      <td>0.346014</td>\n",
       "      <td>41.163084</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM_r131_BAG_L2</td>\n",
       "      <td>-72654.138262</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>257.745216</td>\n",
       "      <td>8780.650135</td>\n",
       "      <td>3.041209</td>\n",
       "      <td>63.317035</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM_r96_BAG_L1</td>\n",
       "      <td>-72655.608282</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>47.997775</td>\n",
       "      <td>222.654808</td>\n",
       "      <td>47.997775</td>\n",
       "      <td>222.654808</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LightGBM_r131_BAG_L1</td>\n",
       "      <td>-72661.922313</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>5.737383</td>\n",
       "      <td>61.007603</td>\n",
       "      <td>5.737383</td>\n",
       "      <td>61.007603</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LightGBM_BAG_L2</td>\n",
       "      <td>-72672.297531</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>255.424848</td>\n",
       "      <td>8753.740922</td>\n",
       "      <td>0.720841</td>\n",
       "      <td>36.407823</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CatBoost_r13_BAG_L1</td>\n",
       "      <td>-72680.646528</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.293932</td>\n",
       "      <td>38.037250</td>\n",
       "      <td>0.293932</td>\n",
       "      <td>38.037250</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>-72691.492496</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.333035</td>\n",
       "      <td>29.595731</td>\n",
       "      <td>1.333035</td>\n",
       "      <td>29.595731</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LightGBM_r15_BAG_L1</td>\n",
       "      <td>-72698.648222</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.965306</td>\n",
       "      <td>38.259015</td>\n",
       "      <td>2.965306</td>\n",
       "      <td>38.259015</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBoost_r89_BAG_L1</td>\n",
       "      <td>-72710.968163</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.304869</td>\n",
       "      <td>15.494084</td>\n",
       "      <td>0.304869</td>\n",
       "      <td>15.494084</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CatBoost_r9_BAG_L1</td>\n",
       "      <td>-72721.913939</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.600352</td>\n",
       "      <td>27.468683</td>\n",
       "      <td>0.600352</td>\n",
       "      <td>27.468683</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoost_r167_BAG_L1</td>\n",
       "      <td>-72726.371575</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>22.891634</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>22.891634</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>-72735.852678</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>3.379681</td>\n",
       "      <td>42.352328</td>\n",
       "      <td>3.379681</td>\n",
       "      <td>42.352328</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CatBoost_r70_BAG_L1</td>\n",
       "      <td>-72740.768167</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.602907</td>\n",
       "      <td>24.576872</td>\n",
       "      <td>0.602907</td>\n",
       "      <td>24.576872</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>-72753.694478</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.160613</td>\n",
       "      <td>24.576001</td>\n",
       "      <td>0.160613</td>\n",
       "      <td>24.576001</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CatBoost_r50_BAG_L1</td>\n",
       "      <td>-72755.991801</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.521612</td>\n",
       "      <td>22.560761</td>\n",
       "      <td>0.521612</td>\n",
       "      <td>22.560761</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CatBoost_r177_BAG_L1</td>\n",
       "      <td>-72757.796818</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>22.847212</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>22.847212</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NeuralNetFastAI_r191_BAG_L2</td>\n",
       "      <td>-72784.409387</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>258.364211</td>\n",
       "      <td>8824.889261</td>\n",
       "      <td>3.660204</td>\n",
       "      <td>107.556161</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LightGBM_r196_BAG_L1</td>\n",
       "      <td>-72804.733726</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>91.777832</td>\n",
       "      <td>420.007644</td>\n",
       "      <td>91.777832</td>\n",
       "      <td>420.007644</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CatBoost_r69_BAG_L1</td>\n",
       "      <td>-72807.804204</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>24.642840</td>\n",
       "      <td>0.209677</td>\n",
       "      <td>24.642840</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CatBoost_r137_BAG_L1</td>\n",
       "      <td>-72812.166025</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.187226</td>\n",
       "      <td>26.861402</td>\n",
       "      <td>0.187226</td>\n",
       "      <td>26.861402</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LightGBM_r161_BAG_L1</td>\n",
       "      <td>-72828.146962</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>6.803094</td>\n",
       "      <td>85.465630</td>\n",
       "      <td>6.803094</td>\n",
       "      <td>85.465630</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>-72837.204400</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.279326</td>\n",
       "      <td>14.572476</td>\n",
       "      <td>0.279326</td>\n",
       "      <td>14.572476</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LightGBM_r188_BAG_L1</td>\n",
       "      <td>-72891.124148</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>3.227483</td>\n",
       "      <td>51.102482</td>\n",
       "      <td>3.227483</td>\n",
       "      <td>51.102482</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>XGBoost_r194_BAG_L1</td>\n",
       "      <td>-72899.093438</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.228529</td>\n",
       "      <td>14.631535</td>\n",
       "      <td>0.228529</td>\n",
       "      <td>14.631535</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>XGBoost_BAG_L2</td>\n",
       "      <td>-72942.141180</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>255.711469</td>\n",
       "      <td>8743.779289</td>\n",
       "      <td>1.007462</td>\n",
       "      <td>26.446189</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NeuralNetFastAI_r102_BAG_L1</td>\n",
       "      <td>-73016.774516</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>185.469967</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>185.469967</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>LightGBMLarge_BAG_L2</td>\n",
       "      <td>-73054.396311</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>256.002130</td>\n",
       "      <td>8766.411779</td>\n",
       "      <td>1.298123</td>\n",
       "      <td>49.078679</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NeuralNetFastAI_r156_BAG_L1</td>\n",
       "      <td>-73083.726356</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.883483</td>\n",
       "      <td>235.817507</td>\n",
       "      <td>0.883483</td>\n",
       "      <td>235.817507</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ExtraTrees_r172_BAG_L1</td>\n",
       "      <td>-73130.205840</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>8.284728</td>\n",
       "      <td>65.775203</td>\n",
       "      <td>8.284728</td>\n",
       "      <td>65.775203</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LightGBMLarge_BAG_L1</td>\n",
       "      <td>-73131.206435</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.184823</td>\n",
       "      <td>33.734397</td>\n",
       "      <td>1.184823</td>\n",
       "      <td>33.734397</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGBoost_r33_BAG_L1</td>\n",
       "      <td>-73214.095855</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.366046</td>\n",
       "      <td>24.924384</td>\n",
       "      <td>0.366046</td>\n",
       "      <td>24.924384</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NeuralNetFastAI_r143_BAG_L1</td>\n",
       "      <td>-73388.742613</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.217153</td>\n",
       "      <td>375.614000</td>\n",
       "      <td>1.217153</td>\n",
       "      <td>375.614000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NeuralNetFastAI_BAG_L1</td>\n",
       "      <td>-73557.947568</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>3.050364</td>\n",
       "      <td>778.017476</td>\n",
       "      <td>3.050364</td>\n",
       "      <td>778.017476</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NeuralNetFastAI_r95_BAG_L1</td>\n",
       "      <td>-73645.711721</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>5.963442</td>\n",
       "      <td>370.719763</td>\n",
       "      <td>5.963442</td>\n",
       "      <td>370.719763</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NeuralNetFastAI_r191_BAG_L1</td>\n",
       "      <td>-73695.722162</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>3.203273</td>\n",
       "      <td>893.101605</td>\n",
       "      <td>3.203273</td>\n",
       "      <td>893.101605</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NeuralNetFastAI_r145_BAG_L1</td>\n",
       "      <td>-73832.019416</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>6.074331</td>\n",
       "      <td>1642.069553</td>\n",
       "      <td>6.074331</td>\n",
       "      <td>1642.069553</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NeuralNetFastAI_r103_BAG_L1</td>\n",
       "      <td>-73903.021749</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>3.068908</td>\n",
       "      <td>739.414387</td>\n",
       "      <td>3.068908</td>\n",
       "      <td>739.414387</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ExtraTreesMSE_BAG_L2</td>\n",
       "      <td>-73907.626798</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>268.740654</td>\n",
       "      <td>8984.701182</td>\n",
       "      <td>14.036647</td>\n",
       "      <td>267.368083</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RandomForest_r39_BAG_L1</td>\n",
       "      <td>-74342.345926</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>10.847727</td>\n",
       "      <td>133.968160</td>\n",
       "      <td>10.847727</td>\n",
       "      <td>133.968160</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>RandomForestMSE_BAG_L2</td>\n",
       "      <td>-74776.010589</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>271.790491</td>\n",
       "      <td>11388.137892</td>\n",
       "      <td>17.086484</td>\n",
       "      <td>2670.804793</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ExtraTrees_r42_BAG_L1</td>\n",
       "      <td>-75980.972201</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>7.987098</td>\n",
       "      <td>54.243201</td>\n",
       "      <td>7.987098</td>\n",
       "      <td>54.243201</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ExtraTreesMSE_BAG_L1</td>\n",
       "      <td>-76193.257067</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>7.478927</td>\n",
       "      <td>60.880453</td>\n",
       "      <td>7.478927</td>\n",
       "      <td>60.880453</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>NeuralNetFastAI_r11_BAG_L1</td>\n",
       "      <td>-76215.504412</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>5.578873</td>\n",
       "      <td>1515.220197</td>\n",
       "      <td>5.578873</td>\n",
       "      <td>1515.220197</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RandomForest_r195_BAG_L1</td>\n",
       "      <td>-76464.995009</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>11.702653</td>\n",
       "      <td>149.068491</td>\n",
       "      <td>11.702653</td>\n",
       "      <td>149.068491</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>RandomForestMSE_BAG_L1</td>\n",
       "      <td>-77363.645271</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>8.049561</td>\n",
       "      <td>153.028691</td>\n",
       "      <td>8.049561</td>\n",
       "      <td>153.028691</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>CatBoost_r9_BAG_L2</td>\n",
       "      <td>-77625.651323</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>254.989763</td>\n",
       "      <td>8748.565766</td>\n",
       "      <td>0.285756</td>\n",
       "      <td>31.232667</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model     score_val              eval_metric  \\\n",
       "0           WeightedEnsemble_L3 -72303.632284  root_mean_squared_error   \n",
       "1        NeuralNetFastAI_BAG_L2 -72352.391728  root_mean_squared_error   \n",
       "2           WeightedEnsemble_L2 -72443.123849  root_mean_squared_error   \n",
       "3             LightGBMXT_BAG_L2 -72488.179841  root_mean_squared_error   \n",
       "4               CatBoost_BAG_L2 -72511.062070  root_mean_squared_error   \n",
       "5          CatBoost_r177_BAG_L2 -72517.248228  root_mean_squared_error   \n",
       "6          LightGBM_r130_BAG_L1 -72606.281582  root_mean_squared_error   \n",
       "7            XGBoost_r98_BAG_L1 -72630.840232  root_mean_squared_error   \n",
       "8          LightGBM_r131_BAG_L2 -72654.138262  root_mean_squared_error   \n",
       "9           LightGBM_r96_BAG_L1 -72655.608282  root_mean_squared_error   \n",
       "10         LightGBM_r131_BAG_L1 -72661.922313  root_mean_squared_error   \n",
       "11              LightGBM_BAG_L2 -72672.297531  root_mean_squared_error   \n",
       "12          CatBoost_r13_BAG_L1 -72680.646528  root_mean_squared_error   \n",
       "13              LightGBM_BAG_L1 -72691.492496  root_mean_squared_error   \n",
       "14          LightGBM_r15_BAG_L1 -72698.648222  root_mean_squared_error   \n",
       "15           XGBoost_r89_BAG_L1 -72710.968163  root_mean_squared_error   \n",
       "16           CatBoost_r9_BAG_L1 -72721.913939  root_mean_squared_error   \n",
       "17         CatBoost_r167_BAG_L1 -72726.371575  root_mean_squared_error   \n",
       "18            LightGBMXT_BAG_L1 -72735.852678  root_mean_squared_error   \n",
       "19          CatBoost_r70_BAG_L1 -72740.768167  root_mean_squared_error   \n",
       "20              CatBoost_BAG_L1 -72753.694478  root_mean_squared_error   \n",
       "21          CatBoost_r50_BAG_L1 -72755.991801  root_mean_squared_error   \n",
       "22         CatBoost_r177_BAG_L1 -72757.796818  root_mean_squared_error   \n",
       "23  NeuralNetFastAI_r191_BAG_L2 -72784.409387  root_mean_squared_error   \n",
       "24         LightGBM_r196_BAG_L1 -72804.733726  root_mean_squared_error   \n",
       "25          CatBoost_r69_BAG_L1 -72807.804204  root_mean_squared_error   \n",
       "26         CatBoost_r137_BAG_L1 -72812.166025  root_mean_squared_error   \n",
       "27         LightGBM_r161_BAG_L1 -72828.146962  root_mean_squared_error   \n",
       "28               XGBoost_BAG_L1 -72837.204400  root_mean_squared_error   \n",
       "29         LightGBM_r188_BAG_L1 -72891.124148  root_mean_squared_error   \n",
       "30          XGBoost_r194_BAG_L1 -72899.093438  root_mean_squared_error   \n",
       "31               XGBoost_BAG_L2 -72942.141180  root_mean_squared_error   \n",
       "32  NeuralNetFastAI_r102_BAG_L1 -73016.774516  root_mean_squared_error   \n",
       "33         LightGBMLarge_BAG_L2 -73054.396311  root_mean_squared_error   \n",
       "34  NeuralNetFastAI_r156_BAG_L1 -73083.726356  root_mean_squared_error   \n",
       "35       ExtraTrees_r172_BAG_L1 -73130.205840  root_mean_squared_error   \n",
       "36         LightGBMLarge_BAG_L1 -73131.206435  root_mean_squared_error   \n",
       "37           XGBoost_r33_BAG_L1 -73214.095855  root_mean_squared_error   \n",
       "38  NeuralNetFastAI_r143_BAG_L1 -73388.742613  root_mean_squared_error   \n",
       "39       NeuralNetFastAI_BAG_L1 -73557.947568  root_mean_squared_error   \n",
       "40   NeuralNetFastAI_r95_BAG_L1 -73645.711721  root_mean_squared_error   \n",
       "41  NeuralNetFastAI_r191_BAG_L1 -73695.722162  root_mean_squared_error   \n",
       "42  NeuralNetFastAI_r145_BAG_L1 -73832.019416  root_mean_squared_error   \n",
       "43  NeuralNetFastAI_r103_BAG_L1 -73903.021749  root_mean_squared_error   \n",
       "44         ExtraTreesMSE_BAG_L2 -73907.626798  root_mean_squared_error   \n",
       "45      RandomForest_r39_BAG_L1 -74342.345926  root_mean_squared_error   \n",
       "46       RandomForestMSE_BAG_L2 -74776.010589  root_mean_squared_error   \n",
       "47        ExtraTrees_r42_BAG_L1 -75980.972201  root_mean_squared_error   \n",
       "48         ExtraTreesMSE_BAG_L1 -76193.257067  root_mean_squared_error   \n",
       "49   NeuralNetFastAI_r11_BAG_L1 -76215.504412  root_mean_squared_error   \n",
       "50     RandomForest_r195_BAG_L1 -76464.995009  root_mean_squared_error   \n",
       "51       RandomForestMSE_BAG_L1 -77363.645271  root_mean_squared_error   \n",
       "52           CatBoost_r9_BAG_L2 -77625.651323  root_mean_squared_error   \n",
       "\n",
       "    pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0      278.316629  10167.588415                0.002995           0.667951   \n",
       "1      258.165104   9724.602528                3.461097        1007.269429   \n",
       "2       71.933454    744.880340                0.003415           0.690237   \n",
       "3      256.148223   8758.280603                1.444216          40.947503   \n",
       "4      254.913498   8751.373427                0.209491          34.040328   \n",
       "5      254.921645   8748.413659                0.217638          31.080560   \n",
       "6        1.677533     35.496591                1.677533          35.496591   \n",
       "7        0.346014     41.163084                0.346014          41.163084   \n",
       "8      257.745216   8780.650135                3.041209          63.317035   \n",
       "9       47.997775    222.654808               47.997775         222.654808   \n",
       "10       5.737383     61.007603                5.737383          61.007603   \n",
       "11     255.424848   8753.740922                0.720841          36.407823   \n",
       "12       0.293932     38.037250                0.293932          38.037250   \n",
       "13       1.333035     29.595731                1.333035          29.595731   \n",
       "14       2.965306     38.259015                2.965306          38.259015   \n",
       "15       0.304869     15.494084                0.304869          15.494084   \n",
       "16       0.600352     27.468683                0.600352          27.468683   \n",
       "17       0.114746     22.891634                0.114746          22.891634   \n",
       "18       3.379681     42.352328                3.379681          42.352328   \n",
       "19       0.602907     24.576872                0.602907          24.576872   \n",
       "20       0.160613     24.576001                0.160613          24.576001   \n",
       "21       0.521612     22.560761                0.521612          22.560761   \n",
       "22       0.154663     22.847212                0.154663          22.847212   \n",
       "23     258.364211   8824.889261                3.660204         107.556161   \n",
       "24      91.777832    420.007644               91.777832         420.007644   \n",
       "25       0.209677     24.642840                0.209677          24.642840   \n",
       "26       0.187226     26.861402                0.187226          26.861402   \n",
       "27       6.803094     85.465630                6.803094          85.465630   \n",
       "28       0.279326     14.572476                0.279326          14.572476   \n",
       "29       3.227483     51.102482                3.227483          51.102482   \n",
       "30       0.228529     14.631535                0.228529          14.631535   \n",
       "31     255.711469   8743.779289                1.007462          26.446189   \n",
       "32       0.859031    185.469967                0.859031         185.469967   \n",
       "33     256.002130   8766.411779                1.298123          49.078679   \n",
       "34       0.883483    235.817507                0.883483         235.817507   \n",
       "35       8.284728     65.775203                8.284728          65.775203   \n",
       "36       1.184823     33.734397                1.184823          33.734397   \n",
       "37       0.366046     24.924384                0.366046          24.924384   \n",
       "38       1.217153    375.614000                1.217153         375.614000   \n",
       "39       3.050364    778.017476                3.050364         778.017476   \n",
       "40       5.963442    370.719763                5.963442         370.719763   \n",
       "41       3.203273    893.101605                3.203273         893.101605   \n",
       "42       6.074331   1642.069553                6.074331        1642.069553   \n",
       "43       3.068908    739.414387                3.068908         739.414387   \n",
       "44     268.740654   8984.701182               14.036647         267.368083   \n",
       "45      10.847727    133.968160               10.847727         133.968160   \n",
       "46     271.790491  11388.137892               17.086484        2670.804793   \n",
       "47       7.987098     54.243201                7.987098          54.243201   \n",
       "48       7.478927     60.880453                7.478927          60.880453   \n",
       "49       5.578873   1515.220197                5.578873        1515.220197   \n",
       "50      11.702653    149.068491               11.702653         149.068491   \n",
       "51       8.049561    153.028691                8.049561         153.028691   \n",
       "52     254.989763   8748.565766                0.285756          31.232667   \n",
       "\n",
       "    stack_level  can_infer  fit_order  \n",
       "0             3       True         53  \n",
       "1             2       True         46  \n",
       "2             2       True         40  \n",
       "3             2       True         41  \n",
       "4             2       True         44  \n",
       "5             2       True         49  \n",
       "6             1       True         23  \n",
       "7             1       True         38  \n",
       "8             2       True         50  \n",
       "9             1       True         13  \n",
       "10            1       True         10  \n",
       "11            2       True         42  \n",
       "12            1       True         18  \n",
       "13            1       True          2  \n",
       "14            1       True         39  \n",
       "15            1       True         22  \n",
       "16            1       True         12  \n",
       "17            1       True         36  \n",
       "18            1       True          1  \n",
       "19            1       True         32  \n",
       "20            1       True          4  \n",
       "21            1       True         24  \n",
       "22            1       True          9  \n",
       "23            2       True         51  \n",
       "24            1       True         34  \n",
       "25            1       True         28  \n",
       "26            1       True         16  \n",
       "27            1       True         30  \n",
       "28            1       True          7  \n",
       "29            1       True         20  \n",
       "30            1       True         26  \n",
       "31            2       True         47  \n",
       "32            1       True         17  \n",
       "33            2       True         48  \n",
       "34            1       True         33  \n",
       "35            1       True         27  \n",
       "36            1       True          8  \n",
       "37            1       True         14  \n",
       "38            1       True         31  \n",
       "39            1       True          6  \n",
       "40            1       True         37  \n",
       "41            1       True         11  \n",
       "42            1       True         21  \n",
       "43            1       True         29  \n",
       "44            2       True         45  \n",
       "45            1       True         35  \n",
       "46            2       True         43  \n",
       "47            1       True         15  \n",
       "48            1       True          5  \n",
       "49            1       True         25  \n",
       "50            1       True         19  \n",
       "51            1       True          3  \n",
       "52            2       True         52  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7640ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T01:04:46.216898Z",
     "iopub.status.busy": "2024-09-06T01:04:46.216488Z",
     "iopub.status.idle": "2024-09-06T01:13:50.918629Z",
     "shell.execute_reply": "2024-09-06T01:13:50.917794Z"
    },
    "papermill": {
     "duration": 544.78934,
     "end_time": "2024-09-06T01:13:50.921615",
     "exception": false,
     "start_time": "2024-09-06T01:04:46.132275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = predictor.predict(test)\n",
    "sub = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\n",
    "\n",
    "sub['price'] = prediction\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9057646,
     "sourceId": 76728,
     "sourceType": "competition"
    },
    {
     "datasetId": 3742543,
     "sourceId": 6478229,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18705.65403,
   "end_time": "2024-09-06T01:13:56.225704",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-05T20:02:10.571674",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
