{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d6752",
   "metadata": {
    "papermill": {
     "duration": 0.006731,
     "end_time": "2024-11-01T03:12:12.276202",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.269471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b1eb7a0",
   "metadata": {
    "papermill": {
     "duration": 0.005215,
     "end_time": "2024-11-01T03:12:12.287375",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.282160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **FOREWORD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35776f6b",
   "metadata": {
    "papermill": {
     "duration": 0.005049,
     "end_time": "2024-11-01T03:12:12.297857",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.292808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a general kernel script for my playground baseline model training purposes. <br>\n",
    "I shall import these scripts in my reference kernel and shall execute them according to the assignment at hand <br>\n",
    "I need to change the eval metric in 3 places- <br>\n",
    "- Utility class\n",
    "- Optuna Tuner class\n",
    "- Model training class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eba051",
   "metadata": {
    "papermill": {
     "duration": 0.005211,
     "end_time": "2024-11-01T03:12:12.308463",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.303252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **PACKAGE INSTALLATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca23087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:12:12.320987Z",
     "iopub.status.busy": "2024-11-01T03:12:12.320596Z",
     "iopub.status.idle": "2024-11-01T03:12:12.332447Z",
     "shell.execute_reply": "2024-11-01T03:12:12.331228Z"
    },
    "papermill": {
     "duration": 0.020796,
     "end_time": "2024-11-01T03:12:12.334771",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.313975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing req_kaggle.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a req_kaggle.txt\n",
    "\n",
    "lightgbm==4.5.0\n",
    "xgboost==2.1.1\n",
    "scikit-learn==1.5.2\n",
    "numpy==1.26.4\n",
    "scipy==1.14.1\n",
    "polars==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d402b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:12:12.347664Z",
     "iopub.status.busy": "2024-11-01T03:12:12.347227Z",
     "iopub.status.idle": "2024-11-01T03:13:26.431855Z",
     "shell.execute_reply": "2024-11-01T03:13:26.429115Z"
    },
    "papermill": {
     "duration": 74.097427,
     "end_time": "2024-11-01T03:13:26.437646",
     "exception": false,
     "start_time": "2024-11-01T03:12:12.340219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import os\n",
    "try:\n",
    "    os.mkdir(f\"/kaggle/working/packages\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "!pip download -q -r req_kaggle.txt -d /kaggle/packages\n",
    "\n",
    "# Downloading polars with NVidia RAPIDS for faster collection process\n",
    "!pip download --extra-index-url=https://pypi.nvidia.com polars[gpu]==1.9.0 -q -d /kaggle/working/polars\n",
    "!pip download --extra-index-url=https://pypi.nvidia.com polars[gpu]==1.12.0 -q -d /kaggle/working/polars1120 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287eb9c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:13:26.455480Z",
     "iopub.status.busy": "2024-11-01T03:13:26.454249Z",
     "iopub.status.idle": "2024-11-01T03:15:19.155179Z",
     "shell.execute_reply": "2024-11-01T03:15:19.153914Z"
    },
    "papermill": {
     "duration": 112.711998,
     "end_time": "2024-11-01T03:15:19.158058",
     "exception": false,
     "start_time": "2024-11-01T03:13:26.446060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "# Downloading relevant AutoML libraries for ready-usage\n",
    "!pip download autogluon.tabular -q -d /kaggle/working/AG\n",
    "!pip download lightautoml -q -d /kaggle/working/LAMA\n",
    "!pip download ray==2.10.0 -q -d /kaggle/working/Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b0a08",
   "metadata": {
    "papermill": {
     "duration": 0.008603,
     "end_time": "2024-11-01T03:15:19.172456",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.163853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fde977b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.186139Z",
     "iopub.status.busy": "2024-11-01T03:15:19.185145Z",
     "iopub.status.idle": "2024-11-01T03:15:19.193556Z",
     "shell.execute_reply": "2024-11-01T03:15:19.192265Z"
    },
    "papermill": {
     "duration": 0.018988,
     "end_time": "2024-11-01T03:15:19.196957",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.177969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing myimports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a myimports.py\n",
    "\n",
    "print(f\"\\n---> Commencing imports-part1\")\n",
    "\n",
    "from gc import collect\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from IPython.display import display_html, clear_output\n",
    "clear_output()\n",
    "import os, sys, logging, re, joblib, ctypes, shutil\n",
    "from copy import deepcopy\n",
    "\n",
    "import xgboost as xgb, lightgbm as lgb, catboost as cb, sklearn as sk, pandas as pd\n",
    "print(f\"---> XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__} | Catboost = {cb.__version__}\")\n",
    "print(f\"---> Sklearn = {sk.__version__}| Pandas = {pd.__version__}\")\n",
    "collect()\n",
    "\n",
    "# General library imports:-\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from gc import collect\n",
    "\n",
    "from os import path, walk, getpid\n",
    "from psutil import Process\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "from IPython.display import display_html, clear_output\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import pandas as pd, numpy as np, os, joblib\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import re\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from colorama import Fore, Style, init\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"---> Imports- part 1 done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60850feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.210104Z",
     "iopub.status.busy": "2024-11-01T03:15:19.209714Z",
     "iopub.status.idle": "2024-11-01T03:15:19.217888Z",
     "shell.execute_reply": "2024-11-01T03:15:19.216868Z"
    },
    "papermill": {
     "duration": 0.017626,
     "end_time": "2024-11-01T03:15:19.220201",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.202575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to myimports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a myimports.py\n",
    "\n",
    "# Importing model and pipeline specifics:-\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Pipeline specifics:-\n",
    "from sklearn.preprocessing import (RobustScaler,\n",
    "                                   MinMaxScaler,\n",
    "                                   StandardScaler,\n",
    "                                   FunctionTransformer as FT,\n",
    "                                   PowerTransformer,\n",
    "                                  )\n",
    "from sklearn.impute import SimpleImputer as SI\n",
    "from sklearn.model_selection import (RepeatedStratifiedKFold as RSKF,\n",
    "                                     StratifiedKFold as SKF,\n",
    "                                     StratifiedGroupKFold as SGKF,\n",
    "                                     KFold,\n",
    "                                     GroupKFold as GKF,\n",
    "                                     RepeatedKFold as RKF,\n",
    "                                     PredefinedSplit as PDS,\n",
    "                                     cross_val_score,\n",
    "                                     cross_val_predict,\n",
    "                                    )\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import VarianceThreshold as VT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "\n",
    "# ML Model training:-\n",
    "from sklearn.metrics import (\n",
    "roc_auc_score, brier_score_loss, accuracy_score, cohen_kappa_score, \n",
    "r2_score, root_mean_squared_error as rmse, mean_squared_error as mse,\n",
    "make_scorer,      \n",
    ")\n",
    "\n",
    "from xgboost import QuantileDMatrix, XGBClassifier as XGBC, XGBRegressor as XGBR\n",
    "from lightgbm import log_evaluation, early_stopping, LGBMClassifier as LGBMC, LGBMRegressor as LGBMR\n",
    "from catboost import CatBoostClassifier as CBC, Pool, CatBoostRegressor as CBR\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier as HGBC, RandomForestClassifier as RFC\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor as HGBR, RandomForestRegressor as RFR\n",
    "from sklearn.linear_model import LogisticRegression as LRC, Ridge, Lasso\n",
    "\n",
    "# Ensemble and tuning:-\n",
    "import optuna\n",
    "from optuna import Trial, trial, create_study\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "\n",
    "# Setting rc parameters in seaborn for plots and graphs-\n",
    "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
    "         \"figure.facecolor\"     : \"#ffffff\",\n",
    "         \"axes.edgecolor\"       : \"#000000\",\n",
    "         \"grid.color\"           : \"#ffffff\",\n",
    "         \"font.family\"          : ['Cambria'],\n",
    "         \"axes.labelcolor\"      : \"#000000\",\n",
    "         \"xtick.color\"          : \"#000000\",\n",
    "         \"ytick.color\"          : \"#000000\",\n",
    "         \"grid.linewidth\"       : 0.75,\n",
    "         \"grid.linestyle\"       : \"--\",\n",
    "         \"axes.titlecolor\"      : '#0099e6',\n",
    "         'axes.titlesize'       : 8.5,\n",
    "         'axes.labelweight'     : \"bold\",\n",
    "         'legend.fontsize'      : 7.0,\n",
    "         'legend.title_fontsize': 7.0,\n",
    "         'font.size'            : 7.5,\n",
    "         'xtick.labelsize'      : 12.5,\n",
    "         'ytick.labelsize'      : 9.0,\n",
    "        }\n",
    "       )\n",
    "\n",
    "# Color printing\n",
    "def PrintColor(text: str, color = Fore.BLUE, style = Style.BRIGHT):\n",
    "    \"Prints color outputs using colorama using a text F-string\"\n",
    "    print(style + color + text + Style.RESET_ALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "732a2c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.233959Z",
     "iopub.status.busy": "2024-11-01T03:15:19.232932Z",
     "iopub.status.idle": "2024-11-01T03:15:19.240873Z",
     "shell.execute_reply": "2024-11-01T03:15:19.239785Z"
    },
    "papermill": {
     "duration": 0.017322,
     "end_time": "2024-11-01T03:15:19.243241",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.225919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to myimports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a myimports.py\n",
    "\n",
    "print(f\"---> Commencing imports-part2\")\n",
    "optuna.logging.set_verbosity = optuna.logging.ERROR\n",
    "optuna.logging.disable_default_handler()\n",
    "print(f\"---> XGBoost = {xgb.__version__} | LightGBM = {lgb.__version__}\")\n",
    "\n",
    "##################################################################\n",
    "# Customizing logging for LGBM\n",
    "class MyLogger:\n",
    "    \"\"\"\n",
    "    This class helps to suppress logs in lightgbm and Optuna\n",
    "    Source - https://github.com/microsoft/LightGBM/issues/6014\n",
    "    \"\"\"\n",
    "\n",
    "    def init(self, logging_lbl: str):\n",
    "        self.logger = logging.getLogger(logging_lbl)\n",
    "        self.logger.setLevel(logging.ERROR)\n",
    "\n",
    "    def info(self, message):\n",
    "        pass\n",
    "\n",
    "    def warning(self, message):\n",
    "        pass\n",
    "\n",
    "    def error(self, message):\n",
    "        self.logger.error(message)\n",
    "\n",
    "l = MyLogger()\n",
    "l.init(logging_lbl = \"lightgbm_custom\")\n",
    "lgb.register_logger(l)\n",
    "\n",
    "##################################################################\n",
    "# Customizing logging for XGBoost\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setLevel(logging.INFO)\n",
    "stdout_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(f'xgb_optimize.log')\n",
    "file_handler.setLevel(logging.ERROR)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "class XGBLogging(xgb.callback.TrainingCallback):\n",
    "    \"\"\"log train logs to file\"\"\"\n",
    "\n",
    "    def __init__(self, epoch_log_interval=100):\n",
    "        self.epoch_log_interval = epoch_log_interval\n",
    "\n",
    "    def after_iteration(self, model, epoch:int,\n",
    "                        evals_log:xgb.callback.TrainingCallback.EvalsLog\n",
    "                        ):\n",
    "\n",
    "        if self.epoch_log_interval <= 0:\n",
    "            pass\n",
    "\n",
    "        elif (epoch %  self.epoch_log_interval == 0):\n",
    "            for data, metric in evals_log.items():\n",
    "                for metric_name, log in metric.items():\n",
    "                    score = log[-1][0] if isinstance(log[-1], tuple) else log[-1]\n",
    "                    logger.info(f\"XGBLogging epoch {epoch} dataset {data} {metric_name} {score}\")\n",
    "\n",
    "        return False\n",
    "\n",
    "# Making sklearn pipeline outputs as dataframe:-\n",
    "from sklearn import set_config\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "print(f\"---> Imports- part 2 done\")\n",
    "\n",
    "print(f\"\\n---> Imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc6345",
   "metadata": {
    "papermill": {
     "duration": 0.005483,
     "end_time": "2024-11-01T03:15:19.254546",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.249063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **TRAINING ELEMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f31b86f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.268382Z",
     "iopub.status.busy": "2024-11-01T03:15:19.267287Z",
     "iopub.status.idle": "2024-11-01T03:15:19.275252Z",
     "shell.execute_reply": "2024-11-01T03:15:19.273783Z"
    },
    "papermill": {
     "duration": 0.017382,
     "end_time": "2024-11-01T03:15:19.277508",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.260126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a training.py\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"\n",
    "    This class creates and uses several utility methods to be used across the code\n",
    "    \"\"\";\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def ScoreMetric(self, ytrue, ypred)-> float:\n",
    "        \"\"\"\n",
    "        This method calculates the metric for the competition\n",
    "        Inputs- ytrue, ypred:- input truth and predictions\n",
    "        Output- float:- competition metric\n",
    "        \"\"\";\n",
    "\n",
    "        score = \\\n",
    "        accuracy_score(\n",
    "            np.uint8(np.round(ytrue, 0)),\n",
    "            np.uint8(np.round(ypred, 0)),\n",
    "        )\n",
    "        return score\n",
    "\n",
    "    def CleanMemory(self):\n",
    "        \"This method cleans the memory off unused objects and displays the cleaned state RAM usage\"\n",
    "\n",
    "        collect();\n",
    "        libc.malloc_trim(0)\n",
    "        pid        = getpid()\n",
    "        py         = Process(pid)\n",
    "        memory_use = py.memory_info()[0] / 2. ** 30\n",
    "        return f\"\\nRAM usage = {memory_use :.4} GB\"\n",
    "\n",
    "    def DisplayAdjTbl(self, *args):\n",
    "        \"\"\"\n",
    "        This function displays pandas tables in an adjacent manner, sourced from the below link-\n",
    "        https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "        \"\"\"\n",
    "\n",
    "        html_str = ''\n",
    "        for df in args:\n",
    "            html_str += df.to_html()\n",
    "        display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "        collect()\n",
    "\n",
    "    def DisplayScores(\n",
    "        self, Scores: pd.DataFrame, TrainScores: pd.DataFrame, methods: list\n",
    "    ):\n",
    "        \"This method displays the scores and their means\"\n",
    "\n",
    "        args = \\\n",
    "        [Scores.style.format(precision = 5).\\\n",
    "         background_gradient(cmap = \"Blues\", subset = methods + [\"Ensemble\"]).\\\n",
    "         set_caption(f\"\\nOOF scores across methods and folds\\n\"),\n",
    "\n",
    "         TrainScores.style.format(precision = 5).\\\n",
    "         background_gradient(cmap = \"Pastel2\", subset = methods).\\\n",
    "         set_caption(f\"\\nTrain scores across methods and folds\\n\")\n",
    "        ];\n",
    "\n",
    "        PrintColor(f\"\\n\\n\\n---> OOF score across all methods and folds\\n\",\n",
    "                   color = Fore.LIGHTMAGENTA_EX\n",
    "                   )\n",
    "        self.DisplayAdjTbl(*args)\n",
    "\n",
    "        print('\\n')\n",
    "        display(Scores.mean().to_frame().\\\n",
    "                transpose().\\\n",
    "                style.format(precision = 5).\\\n",
    "                background_gradient(cmap = \"mako\", axis=1,\n",
    "                                    subset = Scores.columns\n",
    "                                   ).\\\n",
    "                set_caption(f\"\\nOOF mean scores across methods and folds\\n\")\n",
    "               )\n",
    "\n",
    "\n",
    "utils = Utils()\n",
    "collect()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cab751f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.290881Z",
     "iopub.status.busy": "2024-11-01T03:15:19.290189Z",
     "iopub.status.idle": "2024-11-01T03:15:19.297117Z",
     "shell.execute_reply": "2024-11-01T03:15:19.296155Z"
    },
    "papermill": {
     "duration": 0.016501,
     "end_time": "2024-11-01T03:15:19.299651",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.283150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a training.py\n",
    "\n",
    "def MakePermImp(\n",
    "        method, mdl, X, y, ygrp,\n",
    "        myscorer, \n",
    "        n_repeats = 2,\n",
    "        state = 42,\n",
    "        ntop: int = 15,\n",
    "        **params,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function makes the permutation importance for the provided model and returns the importance scores for all features\n",
    "    \n",
    "    Note-\n",
    "    myscorer - scikit-learn -> metrics -> make_scorer object with the corresponding eval metric and relevant details\n",
    "    \"\"\"\n",
    "\n",
    "    cv        = PDS(ygrp)\n",
    "    n_splits  = ygrp.nunique()\n",
    "    drop_cols = [\"Source\", \"id\", \"Id\", \"Label\", \"fold_nb\"]\n",
    "\n",
    "    for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y))):\n",
    "        Xtr  = X.iloc[train_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n",
    "        Xdev = X.iloc[dev_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n",
    "        ytr  = y.loc[Xtr.index]\n",
    "        ydev = y.loc[Xdev.index]\n",
    "\n",
    "        model = clone(mdl)\n",
    "        sel_cols = list(Xdev.columns)\n",
    "        model.fit(Xtr, ytr)\n",
    "\n",
    "        imp_ = permutation_importance(model,\n",
    "                                      Xdev, ydev,\n",
    "                                      scoring = myscorer,\n",
    "                                      n_repeats = n_repeats,\n",
    "                                      random_state = state,\n",
    "                                      )[\"importances_mean\"]\n",
    "        imp_ = pd.Series(index = sel_cols, data = imp_)\n",
    "\n",
    "        display(\n",
    "            imp_.\\\n",
    "            sort_values(ascending = False).\\\n",
    "            head(ntop).\\\n",
    "            to_frame().\\\n",
    "            transpose().\\\n",
    "            style.\\\n",
    "            format(formatter = '{:,.3f}').\\\n",
    "            background_gradient(\"icefire\", axis=1).\\\n",
    "            set_caption(f\"Top {ntop} features\")\n",
    "            )\n",
    "\n",
    "        return imp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546b0778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.313960Z",
     "iopub.status.busy": "2024-11-01T03:15:19.313572Z",
     "iopub.status.idle": "2024-11-01T03:15:19.326058Z",
     "shell.execute_reply": "2024-11-01T03:15:19.324880Z"
    },
    "papermill": {
     "duration": 0.023297,
     "end_time": "2024-11-01T03:15:19.328933",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.305636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a training.py\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"This class trains the provided model on the train-test data and returns the predictions and fitted models\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem_type   : str   = \"binary\", \n",
    "        es             : int   = 100,\n",
    "        target         : str   = \"\",\n",
    "        metric_lbl     : str   = \"auc\",\n",
    "        orig_req       : bool  = False,\n",
    "        orig_all_folds : bool  = False,\n",
    "        drop_cols      : list  = [\"Source\", \"id\", \"Id\", \"Label\", \"fold_nb\"],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Key parameters-\n",
    "        es_iter - early stopping rounds for boosted trees\n",
    "        \"\"\"\n",
    "\n",
    "        self.problem_type   = problem_type\n",
    "        self.es_iter        = es\n",
    "        self.target         = target\n",
    "        self.drop_cols      = drop_cols + [self.target]\n",
    "        self.metric_lbl     = metric_lbl\n",
    "        self.orig_req       = orig_req\n",
    "        self.orig_all_folds = orig_all_folds\n",
    "\n",
    "    def ScoreMetric(self, ytrue, ypred):\n",
    "        \"\"\"\n",
    "        This is the metric function for the competition scoring\n",
    "        \"\"\"\n",
    "\n",
    "        if self.metric_lbl == \"accuracy\":\n",
    "            return accuracy_score(np.uint8(np.round(ytrue, 0)), np.uint8(np.round(ypred)))\n",
    "        elif self.metric_lbl == \"auc\":\n",
    "            return roc_auc_score(ytrue, ypred)\n",
    "        elif self.metric_lbl == \"log_loss\":\n",
    "            return log_loss(ytrue, ypred)\n",
    "\n",
    "    def PlotFtreImp(\n",
    "        self, \n",
    "        ftreimp: pd.Series, \n",
    "        method: str,\n",
    "        ntop: int = 50,\n",
    "        title_specs: dict = {'fontsize': 9,'fontweight' : 'bold','color': '#992600'},\n",
    "        **params,\n",
    "    ):\n",
    "        \"This function plots the feature importances for the model provided\"\n",
    "\n",
    "        print()\n",
    "        fig, ax = plt.subplots(1, 1, figsize = (25, 7.5))\n",
    "\n",
    "        ftreimp.sort_values(ascending = False).\\\n",
    "        head(ntop).\\\n",
    "        plot.bar(ax = ax, color = \"blue\")\n",
    "        ax.set_title(f\"Feature Importances - {method}\", **title_specs)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "    def PostProcessPreds(self, ypred):\n",
    "        \"This method post-processes predictions optionally\"\n",
    "        return np.clip(ypred, a_min = 0.0, a_max = 1.0)\n",
    "\n",
    "    def LoadData(\n",
    "            self, X, y, Xtest,\n",
    "            train_idx : list = [],\n",
    "            dev_idx   : list = [],\n",
    "            ):\n",
    "        \"This method loads the train and test data for the model fold using/ not using the original data\"\n",
    "\n",
    "        if self.orig_req == False:\n",
    "            Xtr  = X.iloc[train_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ytr  = y.iloc[Xtr.index]\n",
    "            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ydev = y.iloc[Xdev.index]\n",
    "\n",
    "        elif self.orig_req == True and self.orig_all_folds == True:\n",
    "            Xtr  = X.iloc[train_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ytr  = y.iloc[Xtr.index]\n",
    "            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ydev = y.iloc[Xdev.index]\n",
    "\n",
    "            orig_x = X.query(\"Source == 'Original'\")[Xtr.columns]\n",
    "            orig_y = y.iloc[orig_x.index]\n",
    "\n",
    "            Xtr = pd.concat([Xtr, orig_x], axis = 0, ignore_index = True)\n",
    "            ytr = pd.concat([ytr, orig_y], axis = 0, ignore_index = True)\n",
    "\n",
    "        elif self.orig_req == True and self.orig_all_folds == False:\n",
    "            Xtr  = X.iloc[train_idx].drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ytr  = y.iloc[Xtr.index]\n",
    "            Xdev = X.iloc[dev_idx].query(\"Source == 'Competition'\").drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "            ydev = y.iloc[Xdev.index]\n",
    "\n",
    "        Xt = Xtest[Xdev.columns]\n",
    "\n",
    "        print(f\"\\n---> Shapes = {Xtr.shape} {ytr.shape} -- {Xdev.shape} {ydev.shape} -- {Xt.shape}\")\n",
    "        return (Xtr, ytr, Xdev, ydev, Xt)\n",
    "    \n",
    "    def MakePreds(self, X, fitted_model):\n",
    "        \"This method creates the model predictions based on the model provided, with optional post-processing\"\n",
    "\n",
    "        if self.problem_type == \"regression\":\n",
    "            return self.PostProcessPreds(fitted_model.predict(X))\n",
    "        elif self.problem_type == \"binary\":\n",
    "            return self.PostProcessPreds(fitted_model.predict_proba(X)[:, 1])\n",
    "        elif self.problem_type == \"multiclass\":\n",
    "            return self.PostProcessPreds(fitted_model.predict_proba(X))\n",
    "\n",
    "    def MakeOrigPreds(\n",
    "            self, orig: pd.DataFrame, fitted_models: list, n_splits : int, ygrp: pd.Series,\n",
    "            ):\n",
    "        \"This method creates the original data predictions separately only if required\"\n",
    "\n",
    "        if self.orig_req == False:\n",
    "            orig_preds = 0\n",
    "\n",
    "        elif self.orig_req == True and self.orig_all_folds == True:\n",
    "            orig_preds = 0\n",
    "            df = orig.drop(self.drop_cols, axis = 1, errors = \"ignore\")\n",
    "\n",
    "            for fitted_model in fitted_models:\n",
    "                orig_preds = orig_preds + (self.MakePreds(df, fitted_model) / n_splits)\n",
    "\n",
    "        elif self.orig_req == True and self.orig_all_folds == False:\n",
    "            len_orig   = orig.shape[0]\n",
    "            orig.index = range(len_orig)\n",
    "            orig_ygrp  = ygrp[-1 * len_orig:]\n",
    "            orig_ygrp.index = range(len_orig)\n",
    "            \n",
    "            orig_preds = np.zeros(len_orig)\n",
    "            for fold_nb, fitted_model in enumerate(fitted_models):\n",
    "                df = \\\n",
    "                orig.iloc[orig_ygrp.loc[orig_ygrp == fold_nb].index].\\\n",
    "                drop(self.drop_cols, axis=1, errors = \"ignore\")\n",
    "                \n",
    "                orig_preds[df.index] = self.MakePreds(df, fitted_model)\n",
    "                del df\n",
    "        return orig_preds\n",
    "\n",
    "    def MakeOfflineModel(\n",
    "        self, X, y, ygrp, Xtest, mdl, method,\n",
    "        test_preds_req   : bool = True,\n",
    "        ftreimp_plot_req : bool = True,\n",
    "        ntop             : int  = 50,\n",
    "        **params,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This function trains the provided model on the dataset and cross-validates appropriately\n",
    "\n",
    "        Inputs-\n",
    "        X, y, ygrp       - training data components (Xtrain, ytrain, fold_nb)\n",
    "        Xtest            - test data (optional)\n",
    "        model            - model object for training\n",
    "        method           - model method label\n",
    "        test_preds_req   - boolean flag to extract test set predictions\n",
    "        ftreimp_plot_req - boolean flag to plot tree feature importances\n",
    "        ntop             - top n features for feature importances plot\n",
    "\n",
    "        Returns-\n",
    "        oof_preds, test_preds - prediction arrays\n",
    "        fitted_models         - fitted model list for test set\n",
    "        ftreimp               - feature importances across selected features\n",
    "        mdl_best_iter         - model average best iteration across folds\n",
    "        \"\"\"\n",
    "\n",
    "        oof_preds     = np.zeros(len(X.loc[X.Source == \"Competition\"]))\n",
    "        orig_preds    = np.zeros(len(X.loc[X.Source == \"Original\"]))\n",
    "        test_preds    = []\n",
    "        mdl_best_iter = []\n",
    "        ftreimp       = 0\n",
    "\n",
    "        scores, tr_scores, fitted_models = [], [], []\n",
    "\n",
    "        if self.orig_req == True:\n",
    "            cv = PDS(ygrp)\n",
    "        elif self.orig_req == False:\n",
    "            X  = X.loc[X.Source == \"Competition\"]\n",
    "            y  = y.iloc[X.index]\n",
    "            cv = PDS(ygrp.iloc[0 : len(X)])\n",
    "\n",
    "        n_splits = ygrp.nunique()\n",
    "\n",
    "        for fold_nb, (train_idx, dev_idx) in tqdm(enumerate(cv.split(X, y))):\n",
    "            Xtr, ytr, Xdev, ydev, Xt = \\\n",
    "            self.LoadData(X, y, Xtest, train_idx, dev_idx)\n",
    "\n",
    "            model = clone(mdl)\n",
    "\n",
    "            if \"CB\" in method:\n",
    "                model.fit(Xtr, ytr,\n",
    "                          eval_set = [(Xdev, ydev)],\n",
    "                          verbose = 0,\n",
    "                          early_stopping_rounds = self.es_iter,\n",
    "                          )\n",
    "                best_iter = model.get_best_iteration()\n",
    "\n",
    "            elif \"LGB\" in method:\n",
    "                model.fit(Xtr, ytr,\n",
    "                          eval_set = [(Xdev, ydev)],\n",
    "                          callbacks = [log_evaluation(0),\n",
    "                                       early_stopping(stopping_rounds = self.es_iter, verbose = False,),\n",
    "                                       ],\n",
    "                          )\n",
    "                best_iter = model.best_iteration_\n",
    "\n",
    "            elif \"XGB\" in method:\n",
    "                model.fit(Xtr, ytr,\n",
    "                          eval_set = [(Xdev, ydev)],\n",
    "                          verbose  = 0,\n",
    "                          )\n",
    "                best_iter = model.best_iteration\n",
    "\n",
    "            else:\n",
    "                model.fit(Xtr, ytr)\n",
    "                best_iter = -1\n",
    "\n",
    "            fitted_models.append(model)\n",
    "\n",
    "            try:\n",
    "                ftreimp += model.feature_importances_\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            dev_preds = self.MakePreds(Xdev, model)\n",
    "            oof_preds[Xdev.index] = dev_preds\n",
    "\n",
    "            train_preds  = self.MakePreds(Xtr, model)\n",
    "            tr_score     = self.ScoreMetric(ytr.values.flatten(), train_preds)\n",
    "            score        = self.ScoreMetric(ydev.values.flatten(), dev_preds)\n",
    "\n",
    "            scores.append(score)\n",
    "            tr_scores.append(tr_score)\n",
    "\n",
    "            nspace = 15 - len(method) - 2 if fold_nb <= 9 else 15 - len(method) - 1\n",
    "            PrintColor(f\"{method} Fold{fold_nb} {' ' * nspace} OOF = {score:.6f} | Train = {tr_score:.6f} | Iter = {best_iter:,.0f} \")\n",
    "            mdl_best_iter.append(best_iter)\n",
    "\n",
    "            if test_preds_req:\n",
    "                test_preds.append(self.MakePreds(Xt, model))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        test_preds    = np.mean(np.stack(test_preds, axis = 1), axis=1)\n",
    "        ftreimp       = pd.Series(ftreimp, index = Xdev.columns)\n",
    "        mdl_best_iter = np.uint16(np.amax(mdl_best_iter))\n",
    "\n",
    "        if ftreimp_plot_req :\n",
    "            print()\n",
    "            self.PlotFtreImp(ftreimp, method = method, ntop = ntop,)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        PrintColor(f\"\\n---> {np.mean(scores):.6f} +- {np.std(scores):.6f} | OOF\", color = Fore.RED)\n",
    "        PrintColor(f\"---> {np.mean(tr_scores):.6f} +- {np.std(tr_scores):.6f} | Train\", color = Fore.RED)\n",
    "\n",
    "        if mdl_best_iter < 0:\n",
    "            pass\n",
    "        else:\n",
    "            PrintColor(f\"---> Max best iteration = {mdl_best_iter :,.0f}\", color = Fore.RED)\n",
    "\n",
    "        if self.orig_req:\n",
    "            print(f\"---> Collecting original predictions\")\n",
    "            orig_preds = self.MakeOrigPreds(X.loc[X.Source == \"Original\"],\n",
    "                                            fitted_models,\n",
    "                                            n_splits,\n",
    "                                            ygrp,\n",
    "                                            )\n",
    "            oof_preds = np.concatenate([oof_preds, orig_preds], axis= 0)\n",
    "        else:\n",
    "            pass\n",
    "        return (fitted_models, oof_preds, test_preds, ftreimp, mdl_best_iter)\n",
    "\n",
    "    def MakeOnlineModel(\n",
    "        self, X, y, Xtest, model, method,\n",
    "        test_preds_req : bool = False,\n",
    "    ):\n",
    "        \"This method refits the model on the complete train data and returns the model fitted object and predictions\"\n",
    "\n",
    "        try:\n",
    "            model.early_stopping_rounds = None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            model.fit(X, y, verbose = 0)\n",
    "        except:\n",
    "            model.fit(X, y,)\n",
    "\n",
    "        oof_preds  = model.predict(X)\n",
    "        if test_preds_req:\n",
    "            test_preds = model.predict(Xtest[X.columns])\n",
    "        else:\n",
    "            test_preds = 0\n",
    "        return (model, oof_preds, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4916060f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.343153Z",
     "iopub.status.busy": "2024-11-01T03:15:19.342740Z",
     "iopub.status.idle": "2024-11-01T03:15:19.351077Z",
     "shell.execute_reply": "2024-11-01T03:15:19.349945Z"
    },
    "papermill": {
     "duration": 0.018263,
     "end_time": "2024-11-01T03:15:19.353450",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.335187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a training.py\n",
    "\n",
    "class OptunaEnsembler:\n",
    "    \"\"\"\n",
    "    This is the Optuna ensemble class-\n",
    "    Source- https://www.kaggle.com/code/arunklenin/ps3e26-cirrhosis-survial-prediction-multiclass\n",
    "    \"\"\";\n",
    "\n",
    "    def __init__(\n",
    "        self, state: int = 42, ntrials: int = 300, \n",
    "        metric_obj: str = \"minimize\", metric_lbl: str = \"accuracy\",\n",
    "        **params\n",
    "    ):\n",
    "        self.study        = None\n",
    "        self.weights      = None\n",
    "        self.random_state = state\n",
    "        self.n_trials     = ntrials\n",
    "        self.direction    = metric_obj\n",
    "        self.metric_lbl   = metric_lbl\n",
    "\n",
    "    def ScoreMetric(self, ytrue, ypred):\n",
    "        \"\"\"\n",
    "        This is the metric function for the competition\n",
    "        \"\"\";\n",
    "\n",
    "        if self.metric_lbl == \"accuracy\":\n",
    "            return accuracy_score(np.uint8(np.round(ytrue, 0)), np.uint8(np.round(ypred)))\n",
    "        elif self.metric_lbl == \"auc\":\n",
    "            return roc_auc_score(ytrue, ypred)\n",
    "        elif self.metric_lbl == \"log_loss\":\n",
    "            return log_loss(ytrue, ypred)\n",
    "\n",
    "    def _objective(\n",
    "        self, trial, y_true, y_preds\n",
    "    ):\n",
    "        \"\"\"\n",
    "        This method defines the objective function for the ensemble\n",
    "        \"\"\";\n",
    "\n",
    "        if isinstance(y_preds, pd.DataFrame) or isinstance(y_preds, np.ndarray):\n",
    "            weights = [trial.suggest_float(f\"weight{n}\", 0.001, 0.999)\n",
    "                       for n in range(y_preds.shape[-1])\n",
    "                      ]\n",
    "            axis = 1\n",
    "\n",
    "        elif isinstance(y_preds, list):\n",
    "            weights = [trial.suggest_float(f\"weight{n}\", 0.001, 0.999)\n",
    "                       for n in range(len(y_preds))\n",
    "                      ]\n",
    "            axis = 0\n",
    "\n",
    "        # Calculating the weighted prediction:-\n",
    "        weighted_pred  = np.average(np.array(y_preds), axis = axis, weights = weights)\n",
    "        score          = self.ScoreMetric(y_true, weighted_pred)\n",
    "        return score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        \"This method fits the Optuna objective on the fold level data\";\n",
    "\n",
    "        optuna.logging.set_verbosity = optuna.logging.ERROR\n",
    "\n",
    "        self.study = \\\n",
    "        optuna.create_study(sampler    = TPESampler(seed = self.random_state),\n",
    "                            pruner     = HyperbandPruner(),\n",
    "                            study_name = \"Ensemble\",\n",
    "                            direction  = self.direction,\n",
    "                           )\n",
    "\n",
    "        obj = partial(self._objective, y_true = y_true, y_preds = y_preds)\n",
    "        self.study.optimize(obj, n_trials = self.n_trials)\n",
    "\n",
    "        if isinstance(y_preds, list):\n",
    "            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "        else:\n",
    "            self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(y_preds.shape[-1])]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        \"This method predicts using the fitted Optuna objective\";\n",
    "\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict';\n",
    "\n",
    "        if isinstance(y_preds, list):\n",
    "            weighted_pred = np.average(np.array(y_preds), axis=0, weights = self.weights)\n",
    "\n",
    "        else:\n",
    "            weighted_pred = np.average(np.array(y_preds), axis=1, weights = self.weights)\n",
    "\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        \"\"\"\n",
    "        This method fits the Optuna objective on the fold data, then predicts the test set\n",
    "        \"\"\";\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "\n",
    "    def weights(self):\n",
    "        \"This method returns the non-normalized weights for all models in a fold\"\n",
    "        return self.weights\n",
    "\n",
    "print()\n",
    "collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b761e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.367331Z",
     "iopub.status.busy": "2024-11-01T03:15:19.366909Z",
     "iopub.status.idle": "2024-11-01T03:15:19.374458Z",
     "shell.execute_reply": "2024-11-01T03:15:19.373394Z"
    },
    "papermill": {
     "duration": 0.017215,
     "end_time": "2024-11-01T03:15:19.376751",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.359536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a training.py\n",
    "\n",
    "def NormWeights(weights: dict, methods: list):\n",
    "    \"This function normalizes the weights and returns a dataframe of normalized weights across folds and models\"\n",
    "\n",
    "    weights = pd.DataFrame.from_dict(weights).T\n",
    "    weights[\"row_sum\"] = weights.sum(axis=1)\n",
    "\n",
    "    for col in weights.columns:\n",
    "        weights[col] = weights[col] / weights[\"row_sum\"]\n",
    "\n",
    "    weights.drop(\"row_sum\", axis = 1, inplace = True, errors = \"ignore\")\n",
    "    weights.columns    = methods\n",
    "    weights.index.name = \"Fold_Nb\"\n",
    "    return weights\n",
    "\n",
    "def MakeEnsemble(target: str, ntrials: int = 300):\n",
    "    \"This function implements the Optuna ensemble on the OOF and test prediction datasets\"\n",
    "\n",
    "    global OOF_Preds, Mdl_Preds\n",
    "\n",
    "    PrintColor(f\"\\n{'=' * 20} ENSEMBLE {'=' * 20}\\n\")\n",
    "\n",
    "    ygrp       = OOF_Preds[\"fold_nb\"]\n",
    "    cv         = PDS(ygrp)\n",
    "    oof_preds  = np.zeros(len(OOF_Preds))\n",
    "    test_preds = []\n",
    "    scores     = []\n",
    "    weights    = {}\n",
    "    drop_cols  = [\"fold_nb\", target, \"Ensemble\"]\n",
    "    n_splits   = ygrp.nunique()\n",
    "\n",
    "    for fold_nb, (_, dev_idx) in tqdm(enumerate(cv.split(OOF_Preds, OOF_Preds[target]))):\n",
    "        Xdev = OOF_Preds.iloc[dev_idx].drop(drop_cols, axis=1, errors = \"ignore\")\n",
    "        ydev = OOF_Preds.loc[dev_idx, target]\n",
    "\n",
    "        ens = OptunaEnsembler(ntrials = ntrials)\n",
    "        ens.fit(ydev, Xdev,)\n",
    "\n",
    "        dev_preds = ens.predict(Xdev)\n",
    "        score     = ens.ScoreMetric(ydev.values, dev_preds)\n",
    "        oof_preds[dev_idx] = dev_preds\n",
    "        test_preds.append(\n",
    "            ens.predict(Mdl_Preds.drop(drop_cols, axis=1, errors = \"ignore\"))\n",
    "        )\n",
    "\n",
    "        PrintColor(f\"---> {score: .6f} | Fold {fold_nb}\", color = Fore.CYAN)\n",
    "        scores.append(score)\n",
    "\n",
    "        weights[f\"Fold{fold_nb}\"] = ens.weights\n",
    "\n",
    "    PrintColor(f\"\\n---> OOF = {np.mean(scores): .6f} +- {np.std(scores): .6f} | Ensemble\",\n",
    "               color = Fore.RED\n",
    "              )\n",
    "\n",
    "    test_preds = np.mean(np.stack(test_preds, axis=1), axis=1,)\n",
    "\n",
    "    OOF_Preds[\"Ensemble\"] = oof_preds\n",
    "    Mdl_Preds[\"Ensemble\"] = test_preds\n",
    "\n",
    "    weights = \\\n",
    "    NormWeights(\n",
    "        weights,\n",
    "        methods = Mdl_Preds.drop(drop_cols, axis=1, errors = \"ignore\").columns\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\\n\")\n",
    "    display(\n",
    "        weights.\\\n",
    "        style.\\\n",
    "        set_caption(\"Normalized weights\").\\\n",
    "        format(precision = 6).\\\n",
    "        set_properties(\n",
    "            props = \"color:red; background-color:white; font-weight: bold; border: maroon dashed 1.6px\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c5018",
   "metadata": {
    "papermill": {
     "duration": 0.005847,
     "end_time": "2024-11-01T03:15:19.388733",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.382886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **STANDARD PREPROCESSOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cfee70b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T03:15:19.402885Z",
     "iopub.status.busy": "2024-11-01T03:15:19.402478Z",
     "iopub.status.idle": "2024-11-01T03:15:19.411540Z",
     "shell.execute_reply": "2024-11-01T03:15:19.410466Z"
    },
    "papermill": {
     "duration": 0.01913,
     "end_time": "2024-11-01T03:15:19.413814",
     "exception": false,
     "start_time": "2024-11-01T03:15:19.394684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a pp.py\n",
    "\n",
    "class Preprocessor():\n",
    "    \"\"\"\n",
    "    This class aims to do the below-\n",
    "    1. Read the datasets\n",
    "    2. In this case, we need to process the original data target column to be compatible with the competition dataset\n",
    "    3. Check information and description\n",
    "    4. Check unique values and nulls\n",
    "    5. Collate starting features \n",
    "    \"\"\";\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train             = pd.read_csv(os.path.join(CFG.ip_path,\"train.csv\"), index_col = 'id')\n",
    "        self.test              = pd.read_csv(os.path.join(CFG.ip_path ,\"test.csv\"), index_col = 'id')\n",
    "        self.target            = CFG.target \n",
    "        self.conjoin_orig_data = True if CFG.nb_orig > 0 else False\n",
    "        self.dtl_preproc_req   = CFG.dtl_preproc_req\n",
    "        self.test_req          = CFG.test_req\n",
    "        self.cv                = cv_selector[CFG.mdlcv_mthd]\n",
    "         \n",
    "        self.original = pd.read_csv(CFG.orig_path)\n",
    "        self.original.index.name = \"id\"    \n",
    "        self.original.index = range(len(self.original))\n",
    "        self.original = self.original[self.train.columns]\n",
    "        self.original[self.target] = np.uint8(np.where(self.original[self.target] == \"No\", 0, 1))\n",
    "        \n",
    "        self.sub_fl = pd.read_csv(os.path.join(CFG.ip_path, \"sample_submission.csv\"))\n",
    "        PrintColor(f\"Data shapes - train-test-original | {self.train.shape} {self.test.shape} {self.original.shape}\")\n",
    "        \n",
    "        for tbl in [self.train, self.original, self.test]:\n",
    "            obj_cols      = tbl.select_dtypes(include = [\"object\", \"category\"]).columns\n",
    "            tbl.columns   = tbl.columns.str.replace(r\"\\(|\\)|\\.|\\?|/|\\s+\",\"\", regex = True)\n",
    "            \n",
    "    def _VisualizeDF(self):\n",
    "        \"This method visualizes the heads for the train, test and original data\"\n",
    "        \n",
    "        PrintColor(f\"\\nTrain set head\", color = Fore.CYAN)\n",
    "        display(self.train.head(5).style.format(precision = 3))\n",
    "        \n",
    "        PrintColor(f\"\\nTest set head\", color = Fore.CYAN)\n",
    "        display(self.test.head(5).style.format(precision = 3))\n",
    "        \n",
    "        PrintColor(f\"\\nOriginal set head\", color = Fore.CYAN)\n",
    "        display(self.original.head(5).style.format(precision = 3))\n",
    "              \n",
    "    def _AddSourceCol(self):\n",
    "        self.train['Source']    = \"Competition\";\n",
    "        self.test['Source']     = \"Competition\";\n",
    "        self.original['Source'] = 'Original';\n",
    "        \n",
    "        self.strt_ftre = self.test.columns;\n",
    "        return self;\n",
    "          \n",
    "    def _CollateInfoDesc(self):\n",
    "        if self.dtl_preproc_req == \"Y\":\n",
    "            PrintColor(f\"\\n{'-' * 20} Information and description {'-' * 20}\\n\", color = Fore.MAGENTA);\n",
    "\n",
    "            # Creating dataset information and description:\n",
    "            for lbl, df in {'Train': self.train, 'Test': self.test, 'Original': self.original}.items():\n",
    "                PrintColor(f\"\\n{lbl} description\\n\");\n",
    "                display(df.describe(percentiles= [0.05, 0.25, 0.50, 0.75, 0.9, 0.95, 0.99]).\\\n",
    "                        transpose().\\\n",
    "                        drop(columns = ['count'], errors = 'ignore').\\\n",
    "                        drop([self.target], axis=0, errors = 'ignore').\\\n",
    "                        style.format(formatter = '{:,.2f}').\\\n",
    "                        background_gradient(cmap = 'Blues')\n",
    "                       );\n",
    "\n",
    "                PrintColor(f\"\\n{lbl} information\\n\");\n",
    "                display(df.info());\n",
    "                collect();\n",
    "        return self;\n",
    "    \n",
    "    def _CollateUnqNull(self):\n",
    "        \n",
    "        if self.dtl_preproc_req == \"Y\":\n",
    "            # Dislaying the unique values across train-test-original:-\n",
    "            PrintColor(f\"\\nUnique and null values\\n\")\n",
    "            _ = pd.concat([self.train[self.strt_ftre].nunique(), \n",
    "                           self.test[self.strt_ftre].nunique(), \n",
    "                           self.original[self.strt_ftre].nunique(),\n",
    "                           self.train[self.strt_ftre].isna().sum(axis=0),\n",
    "                           self.test[self.strt_ftre].isna().sum(axis=0),\n",
    "                           self.original[self.strt_ftre].isna().sum(axis=0)\n",
    "                          ], \n",
    "                          axis=1)\n",
    "            _.columns = ['Train_Nunq', 'Test_Nunq', 'Original_Nunq', \n",
    "                         'Train_Nulls', 'Test_Nulls', 'Original_Nulls'\n",
    "                        ]\n",
    "            display(_.T.style.background_gradient(cmap = 'Blues', axis=1).\\\n",
    "                    format(formatter = '{:,.0f}')\n",
    "                   )\n",
    "            \n",
    "        return self;\n",
    "       \n",
    "    def _ConjoinTrainOrig(self):\n",
    "        if self.conjoin_orig_data :\n",
    "            PrintColor(f\"\\n\\nTrain shape before conjoining with original = {self.train.shape}\")\n",
    "            train = pd.concat([self.train] + [self.original] * CFG.nb_orig, \n",
    "                              axis=0, \n",
    "                              ignore_index = True\n",
    "                             )\n",
    "            PrintColor(f\"Train shape after conjoining with original= {train.shape}\")\n",
    "\n",
    "            train.index = range(len(train))\n",
    "            train.index.name = 'id'\n",
    "\n",
    "        else:\n",
    "            PrintColor(f\"\\nWe are using the competition training data only\")\n",
    "            train = self.train\n",
    "        return train\n",
    "       \n",
    "    def DoPreprocessing(self):\n",
    "        self._VisualizeDF()\n",
    "        self._AddSourceCol()\n",
    "        self._CollateInfoDesc()\n",
    "        self._CollateUnqNull()\n",
    "        self.train = self._ConjoinTrainOrig()\n",
    "        self.train.index = range(len(self.train))\n",
    "        \n",
    "        self.cat_cols  = list(self.test.drop(\"Source\", axis=1).select_dtypes(\"object\").columns)\n",
    "        self.cont_cols = [c for c in self.strt_ftre if c not in self.cat_cols + ['Source']]\n",
    "        return self \n",
    "            \n",
    "collect();\n",
    "print();"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10008389,
     "sourceId": 84895,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 190.54082,
   "end_time": "2024-11-01T03:15:19.840776",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-01T03:12:09.299956",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
